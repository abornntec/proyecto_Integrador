{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df681d03",
   "metadata": {},
   "source": [
    "# Final Model decision and Alternative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba54aae1",
   "metadata": {},
   "source": [
    "## Requirements for running Notebook Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74a467",
   "metadata": {},
   "source": [
    "Anyone trying to run the Notebook will need to provision a service account which has access to Vertex AI platform as a user, to get this access the following steps need to be performed:\n",
    "\n",
    "    1. Log in into GCP (If the user oes not have an account create a new one)\n",
    "    2. Select the project where the Vertex AI instance will be used\n",
    "    3. Open the Services Account option on the left side navigation menu under IAM Section\n",
    "    4. Create a new Service Account\n",
    "    5. Once the account is created go to the IAM section in the left navigation menu\n",
    "    6. Select the user/mail of the new Service account created\n",
    "    7. Edit the accesses and grante Vertex AI user permision to the service account\n",
    "    8. Go back to the service account page\n",
    "    9. Select the Service Account user and go to the key tab\n",
    "    10 Click the create new key button and generate a new key with json format\n",
    "    11. Once the key is created it will be downloaded to the local machine\n",
    "    \n",
    "After getting the service account credentials in local this need to be added to the environmment variables, this is done later in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4105e95",
   "metadata": {},
   "source": [
    "## Install needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9687d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\soyel\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\soyel\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\soyel\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: textstat in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.7.5)\n",
      "Requirement already satisfied: chromadb in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: torch in c:\\users\\soyel\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\soyel\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: hf_xet in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\soyel\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.31.1)\n",
      "Requirement already satisfied: google.auth in c:\\users\\soyel\\anaconda3\\lib\\site-packages (2.40.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: click in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: pyphen in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from textstat) (1.0.32)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from textstat) (75.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (2.8.2)\n",
      "Requirement already satisfied: fastapi==0.115.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.56)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.24)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.38)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (2.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from google.auth) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from google.auth) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from google.auth) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google.auth) (0.4.8)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy matplotlib seaborn nltk textstat chromadb torch sentence-transformers hf_xet transformers accelerate langchain-community huggingface_hub google.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f1b942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "import textstat\n",
    "import re\n",
    "import chromadb\n",
    "import torch\n",
    "import transformers\n",
    "from huggingface_hub import notebook_login\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from google.auth import credentials  # Import the credentials  module\n",
    "from google.auth.transport.requests import Request  # Import Request\n",
    "from google.auth import default #\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credenciales_google.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce63b25",
   "metadata": {},
   "source": [
    "## Prepare Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf4934",
   "metadata": {},
   "source": [
    "### Define the embedding class, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f06ccbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_Embeddings:\n",
    "    def __init__(self):\n",
    "        mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "        print(f\"MPS disponible: {mps_available}\")\n",
    "\n",
    "        cuda_available = torch.cuda.is_available()\n",
    "        print(f\"CUDA disponible: {cuda_available}\")\n",
    "\n",
    "        if cuda_available:\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"Using GPU NVIDIA: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        elif mps_available:\n",
    "            device = torch.device(\"mps\")\n",
    "            print(f\"Usando GPU Apple Silicon via MPS\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Usando CPU\")\n",
    "\n",
    "        print(f\"Dispositivo activo: {device}\")\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            #model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Opcion más ligera\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\", #opcion con 768\n",
    "            model_kwargs={'device': device},\n",
    "            # Este parámetro normaliza cada embedding a longitud 1\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "    def generate_embedding_for_query(self,query):\n",
    "        return self.embedding_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af811c",
   "metadata": {},
   "source": [
    "### Define class to connect to Chroma DB, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a43e96b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chroma_Connection:\n",
    "    def __init__(self,collection_name,persist_directory = \"./chroma_db2\"):\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "        self.generate_embeddings = Generate_Embeddings()\n",
    "\n",
    "    def query_chroma(self, query,n_documents=5):\n",
    "        try:\n",
    "            collection = self.client.get_collection(name=self.collection_name)\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f\"Collection '{collection_name}' not found.  Returning empty results.\"\n",
    "            )\n",
    "            return []\n",
    "        embedded_query = self.generate_embeddings.generate_embedding_for_query(query)\n",
    "        results = collection.query(\n",
    "            query_embeddings=[embedded_query],\n",
    "            n_results=n_documents,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"],  #  Get the text and metadata, and distance\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8325d5",
   "metadata": {},
   "source": [
    "### Define class to do Retrival, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3de998",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retrival:\n",
    "    def __init__(self,collection_name):\n",
    "        self.collection_name = collection_name\n",
    "        self.chroma_connection = Chroma_Connection(self.collection_name)\n",
    "    \n",
    "    def retrival(self,query):\n",
    "        context = self.chroma_connection.query_chroma(query)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f315d",
   "metadata": {},
   "source": [
    "### Define Variable with DB collection name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f0d6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_db_collection=\"C1_RAG_AWS_LENSES\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690c2fdb",
   "metadata": {},
   "source": [
    "### Define function to log in to GCP, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7c6662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcp_token():\n",
    "    try:\n",
    "        SCOPES = ['https://www.googleapis.com/auth/cloud-platform'] # Add all needed scopes\n",
    "        creds, project_id = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "        auth_req = Request()\n",
    "        creds.refresh(auth_req)\n",
    "        access_token = creds.token\n",
    "        return [access_token, project_id]\n",
    "    except Exception as e:\n",
    "        print(f\"Error obtaining credentials: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4a446",
   "metadata": {},
   "source": [
    "### Define class model function from Vertex, reuse from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a881c198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class call_vertex_model:\n",
    "    def __init__(self,model_api,model_name):\n",
    "        self.token, self.project_id = get_gcp_token()\n",
    "        self.model_name = model_name\n",
    "        region = \"us-central1\"\n",
    "        self.model_api = model_api.format(REGION=region,PROJECT_ID=self.project_id,MODEL_ID=self.model_name)\n",
    "\n",
    "    def call_model(self,prompt):\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.token}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Accept\": \"application/json\"\n",
    "            }\n",
    "            payload = {\n",
    "              \"model\": self.model_name,\n",
    "              \"messages\": [\n",
    "              {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                      \"type\": \"text\", \"text\": prompt\n",
    "                    }\n",
    "                  ]\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "            response = requests.post(url=self.model_api, headers=headers, json=payload)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            response_dict = response.json()\n",
    "            generated_text = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Vertex AI endpoint: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9678eb",
   "metadata": {},
   "source": [
    "### Define RAG system class, reuse from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4964d989",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rag_model:\n",
    "    def __init__(self,model_api,model_name,base_prompt,chroma_db_collection):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.call_vertex_model = call_vertex_model(model_api,model_name)\n",
    "        self.retrival = Retrival(chroma_db_collection)\n",
    "\n",
    "    def generate(self,query):\n",
    "        context_content = (self.retrival.retrival(query))[\"documents\"]\n",
    "        prompt = self.base_prompt.format(context_content=context_content, query=query)\n",
    "        generated_text = self.call_vertex_model.call_model(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"response\": generated_text,\n",
    "            \"context\": context_content\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac0355d",
   "metadata": {},
   "source": [
    "### Model 1, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f0b01f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS disponible: False\n",
      "CUDA disponible: False\n",
      "Usando CPU\n",
      "Dispositivo activo: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soyel\\AppData\\Local\\Temp\\ipykernel_9592\\3897626208.py:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embedding_model = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "first_mistral_prompt = \"\"\"\n",
    "    [INST]You are an expert Cloud architect specializing in AWS cloud solutions. Analyze the provided context and the architectural requirements to propose the best AWS-based solution, adhering to AWS Well-Architected Framework principles. Ensure your response uses only AWS services and does not rely on external knowledge beyond the provided context.\n",
    "    Context:\n",
    "        {context_content}\n",
    "    Architectural Requirements:\n",
    "        {query}\n",
    "    Provide your architectural decision in the following format:\n",
    "\n",
    "    1.  **Proposed AWS Architecture:**\n",
    "    2.  **Justification:**\n",
    "    3.  **AWS Services:**\n",
    "    4.  **AWS Only:**\n",
    "    5.  **Context Justification**\n",
    "\n",
    "    If the context lacks sufficient information to make a confident decision, state: \"Insufficient context to provide a confident architectural decision.\" and briefly explain what information is missing.\n",
    "    Do Not use anything outside the proided Context, only services mentioned in the provided context.\n",
    "    [/INST]\n",
    "    **BEGIN ASSISTANT RESPONSE:**\n",
    "    Here's my architectural decision:\n",
    "    [/INST]\n",
    "\"\"\"\n",
    "mistral_model_name = \"mistral-small-2503\"\n",
    "mistral_model_api = \"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/publishers/mistralai/models/{MODEL_ID}:rawPredict\"\n",
    "mistal_model_1 = rag_model(mistral_model_api,mistral_model_name,first_mistral_prompt,chroma_db_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b5d8e",
   "metadata": {},
   "source": [
    "### Model 2, Mistral with different prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eaf4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_mistral_prompt = \"\"\"\n",
    "    [INST]You are an expert Cloud architect specializing in AWS cloud solutions. Your core task is to analyze the provided context and the architectural requirements to propose the most suitable AWS-based solution, strictly adhering to AWS Well-Architected Framework principles.\n",
    "\n",
    "    **CRITICAL CONSTRAINTS (Read Carefully and Adhere Strictly):**\n",
    "        1.  **CONTEXT SOLE SOURCE:** Your entire response, including all proposed AWS services and architectural decisions, **MUST ONLY** be derived from and explicitly supported by the provided `Context`.\n",
    "        2.  **NO EXTERNAL KNOWLEDGE:** You are strictly forbidden from incorporating any external knowledge or AWS services that are not explicitly mentioned and described within the `Context`.\n",
    "        3.  **AWS SERVICES ONLY:** Every AWS service you propose **MUST** have been present in the `Context`. Do not invent or assume services.\n",
    "\n",
    "    Context:\n",
    "        {context_content}\n",
    "\n",
    "    Architectural Requirements:\n",
    "        {query}\n",
    "\n",
    "    Provide your architectural decision in the following exact format. If any section cannot be fully completed based *solely* on the provided Context, acknowledge the missing information within that section or in the final \"Insufficient context\" statement.\n",
    "\n",
    "        1.  **Proposed AWS Architecture:**\n",
    "            [Describe the proposed high-level architecture using only AWS services found in the Context. Be concise.]\n",
    "\n",
    "        2.  **Justification:**\n",
    "            [Explain why this architecture is best, linking directly to the Architectural Requirements and relevant AWS Well-Architected Framework principles (Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, Sustainability) as supported by the Context.]\n",
    "\n",
    "        3.  **AWS Services:**\n",
    "            [List all specific AWS services mentioned in the Proposed Architecture and Justification, ensuring each was present in the provided Context.]\n",
    "\n",
    "        4.  **AWS Only:**\n",
    "            [Confirm explicitly that every AWS service mentioned in this response was found in the provided Context. If you used a service, briefly state where it was mentioned or how its function was described in the Context. This section is a self-validation of Context adherence.]\n",
    "\n",
    "        5.  **Context Justification:**\n",
    "            [Explain precisely how the proposed solution and its components are directly supported by specific details or concepts from the provided Context. Reference sections or key phrases from the Context if possible to demonstrate explicit reliance on the given information.]\n",
    "\n",
    "    If the context lacks sufficient information to make a confident architectural decision based on the strict constraints above, state: \"Insufficient context to provide a confident architectural decision.\" and briefly explain what specific information (e.g., service details, architectural patterns, specific requirements) is missing from the provided context that prevents a complete answer.\n",
    "\n",
    "    [/INST]\n",
    "    **BEGIN ASSISTANT RESPONSE:**\n",
    "        Here's my architectural decision:\n",
    "    [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344f6fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS disponible: False\n",
      "CUDA disponible: False\n",
      "Usando CPU\n",
      "Dispositivo activo: cpu\n"
     ]
    }
   ],
   "source": [
    "mistal_model_2 = rag_model(mistral_model_api,mistral_model_name,second_mistral_prompt,chroma_db_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c373e",
   "metadata": {},
   "source": [
    "### Model 3 LLAMA with intiall prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac5bfe",
   "metadata": {},
   "source": [
    "### Model 4 LLAMA with second prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354edb52",
   "metadata": {},
   "source": [
    "## Preapere evaluation Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9927692",
   "metadata": {},
   "source": [
    "### Create Judge Model, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa0f3071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Judge_Model:\n",
    "    def __init__(self,model_api,model_name):\n",
    "        self.call_vertex_model = call_vertex_model(model_api,model_name)\n",
    "    \n",
    "    def build_judge_prompt(self, question, mistral_response, expected_answer, context):\n",
    "        return f\"\"\"\n",
    "            You are an expert in cloud architecture for financial institutions. Your task is to evaluate a response generated by a language model, given a context, a question (architectural requirement), and an expected answer.\n",
    "\n",
    "            The model was instructed to propose AWS-based solutions that:\n",
    "            - Use **only services found in the provided context**\n",
    "            - Follow the AWS Well-Architected Framework\n",
    "            - Use the following structure: \n",
    "                1. Proposed AWS Architecture\n",
    "                2. Justification\n",
    "                3. AWS Services\n",
    "                4. AWS Only\n",
    "                5. Context Justification\n",
    "            - If the context is insufficient, the model must reply: \"Insufficient context to provide a confident architectural decision\" and explain what's missing.\n",
    "\n",
    "            Below is the evaluation task:\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Architectural Requirement (Question):\n",
    "            {question}\n",
    "\n",
    "            Model-generated answer:\n",
    "            {mistral_response}\n",
    "\n",
    "            Expected answer:\n",
    "            {expected_answer}\n",
    "\n",
    "            Evaluate the model-generated answer using these dimensions (scale 1–5):\n",
    "\n",
    "            - Technical Accuracy\n",
    "            - Clarity\n",
    "            - Completeness\n",
    "\n",
    "            **Important Evaluation Rules**:\n",
    "            - Penalize any use of services not found in the context.\n",
    "            - Penalize if the required structure is not followed.\n",
    "            - Only reward completeness if the answer includes all key components **explicitly relevant to the question**, and provides sufficient detail on how they are used or configured. Including off-topic services or omitting key configuration details should reduce the completeness score.\n",
    "            - Do not reward unnecessary or off-topic information.\n",
    "            - Accuracy should reflect alignment with the Well-Architected Framework and proper AWS service usage.\n",
    "            - Clarity should reflect whether the response is concise, readable, and logically structured.\n",
    "\n",
    "            Provide your evaluation in the following JSON format:\n",
    "\n",
    "            {{\n",
    "            \"accuracy\": <1 to 5>,\n",
    "            \"clarity\": <1 to 5>,\n",
    "            \"completeness\": <1 to 5>,\n",
    "            \"justification\": {{\n",
    "                \"accuracy\": \"Your justification here.\",\n",
    "                \"clarity\": \"Your justification here.\",\n",
    "                \"completeness\": \"Your justification here.\"\n",
    "            }}\n",
    "            }}\n",
    "        \"\"\".strip()\n",
    "\n",
    "    def generate(self, question, base_llm_response, expected_answer, context):\n",
    "        self.base_prompt = self.build_judge_prompt(question, base_llm_response, expected_answer, context)\n",
    "        generated_text = self.call_vertex_model.call_model(self.base_prompt)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad0798b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model_name = \"meta/llama-3.3-70b-instruct-maas\"\n",
    "judge_model_api = \"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi/chat/completions\"\n",
    "judge_model = Judge_Model(judge_model_api,judge_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454ecef",
   "metadata": {},
   "source": [
    "### Define class to extract test results, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e20708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judge_data(judge_text):\n",
    "    try:\n",
    "        match = re.search(r\"```([\\s\\S]*?)```\", judge_text)\n",
    "        if match:\n",
    "            json_str = match.group(1).strip()\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            print(\"No JSON block found between triple backticks.\")\n",
    "            return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected parsing error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0949a7e",
   "metadata": {},
   "source": [
    "### Define function for validating treshhold, reuse from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a99eb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(row, threshold=4.0, min_each=3):\n",
    "    scores = [row['accuracy_score'], row['clarity_score'], row['completeness_score']]\n",
    "    return row['avg_score'] >= threshold and all(score >= min_each for score in scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216474f2",
   "metadata": {},
   "source": [
    "### Defne Evaluation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d867a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluate:\n",
    "    def __init__(self,judge_model,treshhold,min_each):\n",
    "        self.judge_model = judge_model\n",
    "        self.teshhold=treshhold\n",
    "        self.min_each=min_each\n",
    "        \n",
    "    def generate_evaluation(self,evaluated_model,question,expected_answer):\n",
    "        generated_answer_context = evaluated_model.generate(question)\n",
    "        generated_answer = generated_answer_context['response']\n",
    "        context = generated_answer_context['context']\n",
    "        generated_evaluation = self.judge_model.generate(question, generated_answer, expected_answer, context) \n",
    "        evaluation_dict = extract_judge_data(generated_evaluation)\n",
    "        return evaluation_dict\n",
    "    \n",
    "    def evaluate_model(self,evaluated_model,evaluated_model_name,questions_ans_df):\n",
    "        correct_sum = 0\n",
    "        answers = []\n",
    "        for idx, row in tqdm(questions_ans_df.iterrows(), total=questions_ans_df.shape[0]):\n",
    "            question = row['question']\n",
    "            expected_answer = row['expected_answer_pattern']\n",
    "            \n",
    "            evaluation_dict = self.generate_evaluation(evaluated_model,question,expected_answer)\n",
    "            dictionary = {\n",
    "                \"question\":question,\n",
    "                \"accuracy_score\":evaluation_dict.get('accuracy', 0),\n",
    "                \"clarity_score\":evaluation_dict.get('clarity', 0),\n",
    "                \"completeness_score\":evaluation_dict.get('completeness', 0),\n",
    "                \"accuracy_just\":evaluation_dict.get('justification', {}).get('accuracy', ''),\n",
    "                \"clarity_just\":evaluation_dict.get('justification', {}).get('clarity', ''),\n",
    "                \"completeness_just\":evaluation_dict.get('justification', {}).get('completeness', ''),\n",
    "                \"avg_score\":(evaluation_dict.get('accuracy', 0) + evaluation_dict.get('clarity', 0) + evaluation_dict.get('completeness', 0)) / 3.0\n",
    "            }\n",
    "            answers.append(dictionary)\n",
    "            is_correct=is_acceptable(dictionary)\n",
    "            correct_sum=correct_sum+is_correct\n",
    "        total = questions_ans_df.shape[0]\n",
    "        model_accuracy = correct_sum / total\n",
    "        filename = evaluated_model_name+'results.pkl'\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump(answers, file)\n",
    "        \n",
    "        return model_accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9255f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(judge_model,4,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf5198",
   "metadata": {},
   "source": [
    "### Load test questions from pickle data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d692f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions_df = pd.read_pickle('test_questions_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441572a7",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3325dd",
   "metadata": {},
   "source": [
    "We will create a new data dictionary where we will store the accuracy of each model, as evaluating each model is computationally expensive, so we can export the result in a pickl format and at the end graph the evaluation, as well we will store each result in a list of dictionaries so we can see each question evaluated per model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0d76ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('model_performance_dict_results.pkl', 'rb') as file:\n",
    "        model_performance_dict = pickle.load(file)\n",
    "\n",
    "    # Print the loaded list\n",
    "    print(loaded_list)\n",
    "except:\n",
    "    model_performance_dict = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525cd08",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b868d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 14/139 [02:14<18:25,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 39/139 [06:05<16:15,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 92/139 [14:27<07:30,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 97/139 [15:16<07:14, 10.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No JSON block found between triple backticks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 99/139 [15:33<06:13,  9.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 116/139 [18:08<03:26,  8.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 121/139 [18:53<02:45,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [21:40<00:00,  9.36s/it]\n"
     ]
    }
   ],
   "source": [
    "mistal_model_1_accuracy = evaluate.evaluate_model(mistal_model_1,\"mistal_model_1\",test_questions_df)\n",
    "model_performance_dict[\"mistal_model_1_accuracy\"]=mistal_model_1_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c21dd272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mistal_model_1_accuracy': 0.4028776978417266}\n"
     ]
    }
   ],
   "source": [
    "print(model_performance_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d92f1c",
   "metadata": {},
   "source": [
    "#### Load into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5138ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_performance_dict_results.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model_performance_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7a5b6",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03ebf2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 25/139 [05:36<19:15, 10.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 30/139 [06:30<19:12, 10.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 34/139 [07:14<19:19, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 39/139 [08:12<19:19, 11.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 61/139 [12:10<14:15, 10.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 78/139 [15:20<10:56, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON decode error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [26:35<00:00, 11.48s/it]\n"
     ]
    }
   ],
   "source": [
    "mistal_model_2_accuracy = evaluate.evaluate_model(mistal_model_2,\"mistal_model_2\",test_questions_df)\n",
    "model_performance_dict[\"mistal_model_2_accuracy\"]=mistal_model_2_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7483a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mistal_model_1_accuracy': 0.4028776978417266, 'mistal_model_2_accuracy': 0.2733812949640288}\n"
     ]
    }
   ],
   "source": [
    "print(model_performance_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf1a32",
   "metadata": {},
   "source": [
    "#### Load into pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66b4961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_performance_dict_results.pkl'\n",
    "with open(filename, 'wb') as file:\n",
    "    pickle.dump(model_performance_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bc0820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
