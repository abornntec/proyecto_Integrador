{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe802bf5",
   "metadata": {
    "id": "fe802bf5"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52102f4",
   "metadata": {
    "id": "c52102f4"
   },
   "source": [
    "## Model hosting research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1300810",
   "metadata": {
    "id": "d1300810"
   },
   "source": [
    "As mentioned in the previous progress there were problems with the necesary compute power for running the models in our local machines and a exploration of using a Cloud solution was done. After doing some research we have decided to implement Vertex AI leveraging GCP, this will allow to test Mistral and Llama models without worrying about the compute resources in our local using the managed endpoints available as part of the solution.\n",
    "With the managed endpoints we will be able to call the models running in GCP and only pay for the requests made, as we are new users of GCP a 300 USD voucher was generated to us which will make the usage of the models free, however if this does not work the charge for the models is extremly low for the POC purpose of the project. After a succesfull implementation of a RAG system the company will need to perform and evaluation on what is the best way to host the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04449424",
   "metadata": {
    "id": "04449424"
   },
   "source": [
    "## Requirements for running Notebook Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac212131",
   "metadata": {
    "id": "ac212131"
   },
   "source": [
    "Anyone trying to run the Notebook will need to provision a service account which has access to Vertex AI platform as a user, to get this access the following steps need to be performed:\n",
    "\n",
    "    1. Log in into GCP (If the user oes not have an account create a new one)\n",
    "    2. Select the project where the Vertex AI instance will be used\n",
    "    3. Open the Services Account option on the left side navigation menu under IAM Section\n",
    "    4. Create a new Service Account\n",
    "    5. Once the account is created go to the IAM section in the left navigation menu\n",
    "    6. Select the user/mail of the new Service account created\n",
    "    7. Edit the accesses and grante Vertex AI user permision to the service account\n",
    "    8. Go back to the service account page\n",
    "    9. Select the Service Account user and go to the key tab\n",
    "    10 Click the create new key button and generate a new key with json format\n",
    "    11. Once the key is created it will be downloaded to the local machine\n",
    "    \n",
    "After getting the service account credentials in local this need to be added to the environmment variables, this is done later in the notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a413b7",
   "metadata": {
    "id": "82a413b7"
   },
   "source": [
    "## Install needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8362e3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5033,
     "status": "ok",
     "timestamp": 1747711334086,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "b8362e3d",
    "outputId": "f49948d5-88eb-4839-d3e1-23b8b6de3fc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\soyel\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\soyel\\anaconda3\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: seaborn in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\soyel\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: textstat in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.7.5)\n",
      "Requirement already satisfied: chromadb in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.0.8)\n",
      "Requirement already satisfied: torch in c:\\users\\soyel\\anaconda3\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\soyel\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: hf_xet in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\soyel\\anaconda3\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\soyel\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.3.23)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\soyel\\anaconda3\\lib\\site-packages (0.31.1)\n",
      "Requirement already satisfied: google.auth in c:\\users\\soyel\\anaconda3\\lib\\site-packages (2.40.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: click in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: pyphen in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from textstat) (0.17.2)\n",
      "Requirement already satisfied: cmudict in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from textstat) (1.0.32)\n",
      "Requirement already satisfied: setuptools in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from textstat) (75.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (2.8.2)\n",
      "Requirement already satisfied: fastapi==0.115.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.11.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.21.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.9.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (8.2.3)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (6.0.1)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (4.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi==0.115.9->chromadb) (1.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: requests in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.56)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.24)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (2.6.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.3.38)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (2.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from google.auth) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from google.auth) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from google.auth) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google.auth) (0.4.8)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.10.6)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\soyel\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas numpy matplotlib seaborn nltk textstat chromadb torch sentence-transformers hf_xet transformers accelerate langchain-community huggingface_hub google.auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fba930",
   "metadata": {
    "executionInfo": {
     "elapsed": 9017,
     "status": "ok",
     "timestamp": 1747711358166,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "63fba930"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import unicodedata\n",
    "import textstat\n",
    "import re\n",
    "import chromadb\n",
    "import torch\n",
    "import transformers\n",
    "from huggingface_hub import notebook_login\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from google.auth import credentials  # Import the credentials  module\n",
    "from google.auth.transport.requests import Request  # Import Request\n",
    "from google.auth import default #\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./credenciales_google.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccade9",
   "metadata": {
    "id": "d2ccade9"
   },
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306e41f",
   "metadata": {
    "id": "f306e41f"
   },
   "source": [
    "### Define the embedding class, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f6dbe8",
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1747711364024,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "31f6dbe8"
   },
   "outputs": [],
   "source": [
    "class Generate_Embeddings:\n",
    "    def __init__(self):\n",
    "        mps_available = hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()\n",
    "        print(f\"MPS disponible: {mps_available}\")\n",
    "\n",
    "        cuda_available = torch.cuda.is_available()\n",
    "        print(f\"CUDA disponible: {cuda_available}\")\n",
    "\n",
    "        if cuda_available:\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"Using GPU NVIDIA: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        elif mps_available:\n",
    "            device = torch.device(\"mps\")\n",
    "            print(f\"Usando GPU Apple Silicon via MPS\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Usando CPU\")\n",
    "\n",
    "        print(f\"Dispositivo activo: {device}\")\n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            #model_name=\"sentence-transformers/all-MiniLM-L6-v2\",  # Opcion más ligera\n",
    "            model_name=\"sentence-transformers/all-mpnet-base-v2\", #opcion con 768\n",
    "            model_kwargs={'device': device},\n",
    "            # Este parámetro normaliza cada embedding a longitud 1\n",
    "            encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "\n",
    "    def generate_embedding_for_query(self,query):\n",
    "        return self.embedding_model.embed_query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f341eb47",
   "metadata": {
    "id": "f341eb47"
   },
   "source": [
    "### Define class to connect to Chroma DB, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb78648a",
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1747711366012,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "cb78648a"
   },
   "outputs": [],
   "source": [
    "class Chroma_Connection:\n",
    "    def __init__(self,collection_name,persist_directory = \"./chroma_db2\"):\n",
    "        self.persist_directory = persist_directory\n",
    "        self.collection_name = collection_name\n",
    "        self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "        self.generate_embeddings = Generate_Embeddings()\n",
    "\n",
    "    def query_chroma(self, query,n_documents=5):\n",
    "        try:\n",
    "            collection = self.client.get_collection(name=self.collection_name)\n",
    "        except ValueError:\n",
    "            print(\n",
    "                f\"Collection '{collection_name}' not found.  Returning empty results.\"\n",
    "            )\n",
    "            return []\n",
    "        embedded_query = self.generate_embeddings.generate_embedding_for_query(query)\n",
    "        results = collection.query(\n",
    "            query_embeddings=[embedded_query],\n",
    "            n_results=n_documents,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"],  #  Get the text and metadata, and distance\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ca2d3",
   "metadata": {
    "id": "7e8ca2d3"
   },
   "source": [
    "### Define function to do Retrival, reused from previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63124168",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1747711368000,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "63124168"
   },
   "outputs": [],
   "source": [
    "def retrival(query):\n",
    "    collection_name = \"C1_RAG_AWS_LENSES\"\n",
    "    context_retrival = Chroma_Connection(collection_name)\n",
    "    context = context_retrival.query_chroma(query)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7f1276",
   "metadata": {
    "id": "bc7f1276"
   },
   "source": [
    "### Define intial prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fec975",
   "metadata": {
    "id": "26fec975"
   },
   "source": [
    "For next iteration as we will be testing different prompts and different models it is important to define this is the initial prompt for mistral that will set our base ground for evaluating different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dcefe5",
   "metadata": {
    "id": "d9dcefe5"
   },
   "source": [
    "The first prompt we will be evaluating with mistral model will be the following:\n",
    "\n",
    "    [INST]You are an expert Cloud architect specializing in AWS cloud solutions. Analyze the provided context and the architectural requirements to propose the best AWS-based solution, adhering to AWS Well-Architected Framework principles. Ensure your response uses only AWS services and does not rely on external knowledge beyond the provided context.\n",
    "    Context:\n",
    "        {context_content}\n",
    "    Architectural Requirements:\n",
    "        {query}\n",
    "    Provide your architectural decision in the following format:\n",
    "\n",
    "    1.  **Proposed AWS Architecture:**\n",
    "    2.  **Justification:**\n",
    "    3.  **AWS Services:**\n",
    "    4.  **AWS Only:**\n",
    "\n",
    "    If the context lacks sufficient information to make a confident decision, state: \"Insufficient context to provide a confident architectural decision.\" and briefly explain what information is missing.\n",
    "\n",
    "    [/INST]\n",
    "    **BEGIN ASSISTANT RESPONSE:**\n",
    "    Here's my architectural decision:\n",
    "    [/INST]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8366a9",
   "metadata": {
    "id": "5d8366a9"
   },
   "source": [
    "### Define function to log in to GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92f5bc7",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747711371805,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "d92f5bc7"
   },
   "outputs": [],
   "source": [
    "def get_gcp_token():\n",
    "    try:\n",
    "        SCOPES = ['https://www.googleapis.com/auth/cloud-platform'] # Add all needed scopes\n",
    "        creds, project_id = default(scopes=[\"https://www.googleapis.com/auth/cloud-platform\"])\n",
    "        auth_req = Request()\n",
    "        creds.refresh(auth_req)\n",
    "        access_token = creds.token\n",
    "        return [access_token, project_id]\n",
    "    except Exception as e:\n",
    "        print(f\"Error obtaining credentials: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e99f4b0",
   "metadata": {
    "id": "1e99f4b0"
   },
   "source": [
    "### Define class model function from Vertex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "720b7a3f",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1747712581576,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "720b7a3f"
   },
   "outputs": [],
   "source": [
    "class call_vertex_model:\n",
    "    def __init__(self,model_api,model_name):\n",
    "        self.token, self.project_id = get_gcp_token()\n",
    "        self.model_name = model_name\n",
    "        region = \"us-central1\"\n",
    "        self.model_api = model_api.format(REGION=region,PROJECT_ID=self.project_id,MODEL_ID=self.model_name)\n",
    "\n",
    "    def call_model(self,prompt):\n",
    "        try:\n",
    "            headers = {\n",
    "                \"Authorization\": f\"Bearer {self.token}\",\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Accept\": \"application/json\"\n",
    "            }\n",
    "            payload = {\n",
    "              \"model\": self.model_name,\n",
    "              \"messages\": [\n",
    "              {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                      \"type\": \"text\", \"text\": prompt\n",
    "                    }\n",
    "                  ]\n",
    "                }\n",
    "              ]\n",
    "            }\n",
    "            response = requests.post(url=self.model_api, headers=headers, json=payload)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            response_dict = response.json()\n",
    "            generated_text = response_dict[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return generated_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling Vertex AI endpoint: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba8a4d",
   "metadata": {
    "id": "30ba8a4d"
   },
   "source": [
    "### Define RAG system class, reuse some parts of the previous progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "120d0c05",
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1747712582607,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "120d0c05"
   },
   "outputs": [],
   "source": [
    "class rag_model:\n",
    "    def __init__(self,model_api,model_name,base_prompt):\n",
    "        self.base_prompt = base_prompt\n",
    "        self.call_vertex_model = call_vertex_model(model_api,model_name)\n",
    "\n",
    "    def generate(self,query):\n",
    "        context_content = (retrival(query))[\"documents\"]\n",
    "        prompt = self.base_prompt.format(context_content=context_content, query=query)\n",
    "        generated_text = self.call_vertex_model.call_model(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"response\": generated_text,\n",
    "            \"context\": context_content\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed1043f",
   "metadata": {
    "id": "eed1043f"
   },
   "source": [
    "### Create first full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51e6a296",
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "ok",
     "timestamp": 1747712610104,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "51e6a296"
   },
   "outputs": [],
   "source": [
    "first_base_prompt = \"\"\"\n",
    "    [INST]You are an expert Cloud architect specializing in AWS cloud solutions. Analyze the provided context and the architectural requirements to propose the best AWS-based solution, adhering to AWS Well-Architected Framework principles. Ensure your response uses only AWS services and does not rely on external knowledge beyond the provided context.\n",
    "    Context:\n",
    "        {context_content}\n",
    "    Architectural Requirements:\n",
    "        {query}\n",
    "    Provide your architectural decision in the following format:\n",
    "\n",
    "    1.  **Proposed AWS Architecture:**\n",
    "    2.  **Justification:**\n",
    "    3.  **AWS Services:**\n",
    "    4.  **AWS Only:**\n",
    "    5.  **Context Justification**\n",
    "\n",
    "    If the context lacks sufficient information to make a confident decision, state: \"Insufficient context to provide a confident architectural decision.\" and briefly explain what information is missing.\n",
    "    Do Not use anything outside the proided Context, only services mentioned in the provided context.\n",
    "    [/INST]\n",
    "    **BEGIN ASSISTANT RESPONSE:**\n",
    "    Here's my architectural decision:\n",
    "    [/INST]\n",
    "\"\"\"\n",
    "mistral_model_name = \"mistral-small-2503\"\n",
    "mistral_model_api = \"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/publishers/mistralai/models/{MODEL_ID}:rawPredict\"\n",
    "mistral_first_prompt = rag_model(mistral_model_api,mistral_model_name,first_base_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ddba70",
   "metadata": {
    "id": "64ddba70"
   },
   "source": [
    "### Test the RAG system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44286ba0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 423,
     "status": "ok",
     "timestamp": 1747712611672,
     "user": {
      "displayName": "Andre Maximiliano Hernández Bornn",
      "userId": "18435446262227756454"
     },
     "user_tz": 360
    },
    "id": "44286ba0",
    "outputId": "ba053e71-81e1-4b0f-b4b8-4d487409c820"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS disponible: False\n",
      "CUDA disponible: True\n",
      "Using GPU NVIDIA: NVIDIA GeForce RTX 3070 Ti\n",
      "Total Memory: 8.59 GB\n",
      "Dispositivo activo: cuda\n",
      "Response:  **1. Proposed AWS Architecture:**\n",
      "\n",
      "    - **High-Performance Compute Layer:** Utilize EC2 instances with Enhanced Networking (ENA Express) to ensure sub-millisecond latency for transaction processing.\n",
      "    - **Data Storage Layer:** Use Amazon Kinesis Data Streams for real-time data ingestion and processing, ensuring data immutability and durability.\n",
      "    - **Data Processing Layer:** Implement Amazon EMR with Apache Spark for real-time data processing and analytics.\n",
      "    - **Data Storage and Auditing:** Store processed data in Amazon S3 with versioning enabled for immutability and auditing purposes.\n",
      "    - **Networking:** Use Amazon VPC with dedicated Direct Connect for low-latency and secure connectivity.\n",
      "    - **Monitoring and Scaling:** Use Amazon CloudWatch for monitoring performance metrics and AWS Auto Scaling to handle varying workloads.\n",
      "\n",
      "**2. Justification:**\n",
      "\n",
      "    - **Performance Efficiency:** EC2 instances with ENA Express provide the necessary low-latency performance required for real-time trading data processing.\n",
      "    - **Data Immutability:** Amazon Kinesis Data Streams and Amazon S3 with versioning ensure data immutability, which is crucial for auditing and compliance.\n",
      "    - **Scalability and Reliability:** Amazon EMR with Apache Spark can handle large-scale data processing efficiently, and AWS Auto Scaling ensures the system can handle varying workloads without performance degradation.\n",
      "    - **Security and Compliance:** Amazon VPC with Direct Connect provides a secure and low-latency network environment, and Amazon S3 with versioning ensures data integrity and compliance with regulatory requirements.\n",
      "\n",
      "**3. AWS Services:**\n",
      "\n",
      "    - **Compute:** Amazon EC2 with ENA Express\n",
      "    - **Data Ingestion and Processing:** Amazon Kinesis Data Streams, Amazon EMR with Apache Spark\n",
      "    - **Data Storage:** Amazon S3 with versioning\n",
      "    - **Networking:** Amazon VPC, AWS Direct Connect\n",
      "    - **Monitoring and Scaling:** Amazon CloudWatch, AWS Auto Scaling\n",
      "\n",
      "**4. AWS Only:**\n",
      "\n",
      "    - The proposed architecture relies solely on AWS services, ensuring full integration and support within the AWS ecosystem.\n",
      "\n",
      "**5. Context Justification:**\n",
      "\n",
      "    - The context emphasizes the need for sub-millisecond latency and data immutability, which are addressed by using EC2 instances with ENA Express and Amazon Kinesis Data Streams with Amazon S3 versioning.\n",
      "    - The context also highlights the importance of performance efficiency and scalability, which are achieved through the use of Amazon EMR with Apache Spark and AWS Auto Scaling.\n",
      "    - The focus on security and compliance is met by using Amazon VPC with Direct Connect and Amazon S3 with versioning.\n",
      "Context:  [['resources - financial services industry lensresources - financial services industry lensdocumentationaws well-architectedaws well-architected frameworkdocumentation blogswhitepaperspartner solutionsreference architecturesvideosresources refer following resources learn best practices related performance efficiency financial services industry solutions . documentation blogs rethinking low latency trade value proposition using local zones improve frtb ’ internal model approach implementation using apache spark emr cloud increases flexibility trading risk infrastructure frtb compliance crypto market-making latency ec2 shared placement groups cloudfront fsi service spotlight automating scaling chaos engineering using fault injection service whitepapers financial services grid computing partner solutions stac-m3 benchmark results : low-latency tick analytics made easy scaling managing tibco datasynapse gridserver reference architectures high performance computing running sas grid general hpc architecture videos nyse : protecting markets real-time processing nasdaq : moving mission-critical , low-latency workloads hsbc uses serverless process millions transactions real time finra collects , analyzes billions brokerage transaction records daily using finra operates pb-scale analytics lakes athena morgan stanley leveraged ec2 spot scale demand risk calculations using hpc spot instances morgan stanley dbs bank : scalable serverless compute grid temenos : building serverless banking software scale helped financial services company adopt serverless architecture effectively scale financial services company addressed 4x increase call volume cloud', 'performance efficiency - financial services industry lensperformance efficiency - financial services industry lensdocumentationaws well-architectedaws well-architected frameworkdesign principlesdefinitionsperformance efficiency performance efficiency pillar focuses efficient use resources meet requirements , maintain improve efficiency demands change technologies evolve . key topics include selecting right infrastructure based workload requirements , monitoring performance , making informed decisions maintain efficiency . performance optimization continuous , data-driven process confirming business requirements , monitoring measuring workload performance , identifying under-performing components adjusting infrastructure meet evolving requirements . reviewing choices cyclical basis , take advantage continually evolving cloud . design principles addition design principles well-architected framework whitepaper , following design principles help achieve performance efficiency financial services workloads . consider internal external requirements regulators expect financial services institutions define operational performance objectives workloads , implement policies achieve objectives . regulators may also impose key performance indicator ( kpi ) requirements systemically-important workloads , open banking interfaces , trading transaction reporting expect institutions monitor report compliance requirements , penalties breaches . objectives must define qualitative quantitative measures operational performance thereby explicitly state performance standards workload intends meet . architect performance-driven workloads financial services workloads , example high-frequency trading systems risk calculation engines , particularly performance sensitive , factors speed completion latency response directly impacting profitability system . systems considerations like need prioritize performance factors cost-efficiency reliability , considering trade-offs required achieve performance goals also preserving non-functional requirements transactional consistency recoverability . see trade-offs section pillar detail . use managed services leverage cloud services allow teams use wide range technologies , experiment options achieve performance goals , maintaining overall control . reduce time takes configure , invest operations on-going management , reducing operational overhead using right tool job . definitions focus following areas achieve performance efficiency cloud : selection review monitoring trade-offs gather aspects architecture data-driven approach building high-performance architecture , high-level design selection configuration infrastructure services components compute storage networking . reviewing architectural choices regular basis helps take advantage continually evolving cloud capabilities match workload requirements available services features . monitoring performance workload continuously makes aware deviances expected performance , able take timely action . also important plan future performance system performing load tests projected future loads , running game days exceptional circumstances order understand behavior limits system . performance degrade unexpectedly workloads grow . however , aware constraints places testing type , running load tests web services initiate security mechanisms . information , see elastic compute cloud testing policy . particular , penetration testing run permitted services distributed denial service ( ddos ) testing must performed pre-approved partner . finally , make trade-offs architecture improve performance , using compression reduce size stored transiting network , caching frequently used dedicated services relaxing consistency requirements , prioritizing important requirements .', 'fsiperf06 : make trade-offs architecture ? - financial services industry lensfsiperf06 : make trade-offs architecture ? - financial services industry lensdocumentationaws well-architectedaws well-architected frameworkfsiperf06-bp01 understand priorities architect meet fsiperf06 : make trade-offs architecture ? financial services workloads often make trade-offs architecture meet important goals kpis , performance system deemed important factors , vice-versa . fsiperf06-bp01 understand priorities architect meet example , low-latency trading system needs preserve performance system factors , prepared compromise cost infrastructure meet goals . situation still important compromise availability , may require significant investment parallel , independent , deployments example independent deployment application stack multiple availability zones regions rather failover architecture . within workload may necessary trade-off persistent capacity elasticity make sure application always ability handle peak workloads without needing timed reactive scaling . consider much peak workload need able service time . choosing services consider performance determinism . serverless services like lambda fargate bring significant performance benefits due ability scale elastically demand , without intervention , often coupled less fine control underlying environment , example cpu clock speed , introduce element variability workload performance . workload performance must consistent possible , consider using ec2 , get widest choice , greatest level control , production environment . example , using ec2 directly enables use ena express , increase network throughput reduce latency , brings restrictions ec2 instances support feature . consider trade-offs application architecture . example , preserve network latency may choose use certain services configurations complex implement maintain , offer better performance , using vpc peering instead transit gateway minimize number network hops critical traffic . optimal connectivity on-premises workloads consider best position direct connect gateway bring closest sensitive workloads .', 'characteristics - analytics lenscharacteristics - analytics lensdocumentationaws well-architectedaws well-architected frameworkcharacteristics scalable throughput : real-time analytics , plan resilient stream storage infrastructure adapt changes rate flowing stream . scaling typically performed administrative application monitors shard partition data-handling metrics . dynamic stream processor consumption collaboration : stream processors consumers automatically discover newly added kinesis shards kafka partitions , distribute equitably across available resources process independently collaboratively consumption group ( kinesis application name , kafka consumer group ) . durable : real-time streaming systems provide high availability durability . example , kinesis streams managed streaming apache kafka ( msk ) replicate across availability zones providing high durability streaming applications need . replay-ability : stream storage systems provide ordering records within shards partitions , well ability independently read replay records order stream processors consumers . fault-tolerance , checkpoint , replay : checkpointing refers recording farthest point stream records consumed processed . consuming application crashes , resume reading stream point instead start beginning . loosely coupled integration : key benefit streaming applications construct loose coupling . value loose coupling ability stream ingestion , stream producers , stream processors , stream consumers act behave independently one another . examples include ability scale consumers outside producer configuration adding additional stream processors consumers receive stream topic existing stream processors consumers , perform different actions . allow multiple processing applications parallel : ability multiple applications consume stream concurrently essential characteristic stream processing system . example , might one application updates real-time dashboard another archives redshift . want applications consume stream concurrently independently . messaging semantics : distributed messaging system , components might fail independently . different messaging systems implement different semantic guarantees producer consumer case failure . common message delivery guarantees implemented : : messages could delivered , lost , never redelivered least : message might delivered consumer exactly : message delivered exactly depending application needs , choose message delivery system supports one required semantics . security : streaming ingest processing systems must secure default . must grant access using principal least privilege streaming apis infrastructure , encrypt rest transit . kinesis streams msk configured use iam policies grant least privilege access . stream storage particular , allow encryption transit producers consumers , encryption rest .', \"reference architecture - analytics lensreference architecture - analytics lensdocumentationaws well-architectedaws well-architected frameworkconfiguration notesstreaming application guidelinesdevelopmentperformanceoperationssecurityreference architecture figure 4 : streaming analytics reference architecture preceding streaming reference architecture diagram segmented previously described components streaming scenarios : sources stream ingestion producers stream storage stream processing consumers downstream destinations , portions , reference architecture used workloads application modernization microservices , streaming etl , ingest , real-time inventory , recommendations , fraud detection . section , identify layer components shown preceding diagram specific examples . examples intended exhaustive list , rather attempt describe popular options . subsequent configuration notes section provides recommendations considerations implementing streaming scenarios . review five core components streaming architecture first , discuss specialized flows . sources : number potential sources millions . examples include application logs , mobile apps applications rest apis , iot sensors , existing application databases ( rdbms , nosql ) metering records . stream ingestion producers : multiple sources generate continually might amount terabytes per day . toolkits , libraries , sdks used develop custom stream producers streaming storage . contrast custom developed producers , examples pre-built producers include kinesis agent , change capture ( cdc ) solutions , kafka connect source connectors . streaming storage : kinesis streams , msk , self-managed apache kafka examples stream storage options ingesting , processing , storing large streams records events . streaming storage implementations modeled idea distributed , immutable commit log . events stored configurable duration ( hours days months , even permanently cases ) . stored , events available client . stream processing consumers : real-time streams processed sequentially incrementally record-by-record basis sliding time windows using variety services . , put another way , particular domain-specific logic resides computed . managed service apache flink apache flink managed service apache flink studio , process analyze streaming using standard sql serverless way . service allows quickly author run sql queries streaming sources perform time series analytics , feed real-time dashboards , create real-time metrics . work emr environment , process streaming using multiple options— apache flink spark structured streaming . finally , options lambda , third-party integrations , build-your-own custom applications using sdks , libraries , open-source libraries connectors consuming kinesis streams , msk , apache kafka . downstream destinations : persisted durable storage serve variety use cases including ad hoc analytics search , machine learning , alerts , science experiments , additional custom actions . special note flow lanes noted asterisk ( * ) . two examples involve bidirectional flow layer # 3 streaming storage . first example bidirectional flow in-stream etl stream processor ( # 4 ) uses one raw event sources stream storage ( # 3 ) performs , filtering , aggregations , joins , etc. , writes results back streaming storage refined ( , curated , hydrated ) result stream topic ( # 3 ) used different stream processor downstream consumer . second bidirectional flow example ubiquitous application modernization microservice design ( # 2 ) often use streaming storage layer ( # 3 ) decoupled microservice interaction . key takeaway presume streaming event flows exclusively left-to-right time reference architecture diagram . configuration notes explored far , know streaming architects options implementing particular components stack , example , different options streaming storage , streaming ingest , streaming producers . ’ impractical provide in-depth recommendations layer ’ options whitepaper , high-level concepts consider guide posts , present next . in-depth analysis particular layer design , consider exploring provided links within following guidelines . streaming application guidelines determine business requirements first . ’ always best practice practical focus workload ’ particular needs first , rather starting feature-by-feature comparison technical options . example , often see organizations prioritizing technical feature vs. technical feature b determining workload ’ requirements . wrong order . determine workload ’ requirements first wide variety purpose-built options streaming architecture layer best match requirements . technical comparisons second . business requirements clearly established , next step match business requirements technical options offer best chance success . example , team technical operators , serverless might good option . technical questions workload might whether require large number independent stream processors consuming applications , , one vs. many stream processors consumers . kind manual automatic scaling options available match business requirement throughput , latency , sla , rpo , rto objectives ? desire use open source-based solutions ? security options well integrate existing security postures ? one path easier straightforward migrate versus another , example , self-managed apache kafka msk . learn options various layers reference architecture stack , refer following : streaming ingest producers — workload-dependent use service integrations , iot core , cloudwatch logs events , migration service ( dms ) , third-party integrations ( refer writing kinesis streams kinesis kinesis streams developer guide ) . streaming storage — kinesis streams , firehose , msk , apache kafka ( refer best practices managed managed streaming apache kafka developer guide ) . stream processing consumers — managed service apache flink apache flink , firehose , lambda , open source proprietary sdks ( refer advanced topics kinesis streams consumers kinesis streams developer guide best practices managed service apache flink apache flink managed service apache flink developer guide ) . information , refer build modern streaming architectures whitepaper . remember separation concerns . separation concerns application design principle promotes segmenting application distinct , particular area concerns . example , application might require stream processors consumers performing aggregation computation addition recording computation results downstream destination . tempting clump concerns one stream processors consumers , recommended consider separation instead . ’ often better segment isolate multiple stream processors consumers operation monitoring , performance tuning isolation , reducing downtime blast radius . development existing desired skills match . realizing value streaming architectures difficult often new endeavor various roles within organizations . increase chances success aligning team ’ existing skillsets , desired skillsets , wherever possible . example , team familiar java prefer different language , python go ? team prefer graphical user interface writing deploying code ? work backwards existing skill resources preferences appropriate options component . build vs. buy ( write use off-the-shelf ) . consider whether integration components already exists must write . perhaps options available . many teams new streaming incorrectly assume everything must written scratch . instead , consider services kafka connect connectors inbound outbound traffic , lambda , firehose . performance aggregate records sending stream storage increased throughput . using kinesis , msk , kafka , ensure messages accumulated producer side sending stream storage . also referred batching records increase throughput , cost increased latency . working kinesis streams , use kinesis client library ( kcl ) de-aggregate records . kcl takes care many complex tasks associated distributed computing , load balancing across multiple instances , responding instance failures , checkpointing processed records , reacting re-sharding . initial planning adjustment shards partitions . common mechanism scale stream storage stream processors consumers number configured shards ( kinesis streams ) partitions ( apache kafka , msk ) particular stream . common element across kinesis streams , msk , apache kafka , options scaling ( ) number shards partitions vary . kinesis streams developer guide : resharding stream apache kafka documentation - operations : expanding cluster ( also applicable msk ) managed streaming apache kafka developer guide : using linkedin 's cruise control apache kafka msk ( partition rebalancing ) use spot instances automatic scaling process streaming cost effectively . also process using lambda kinesis , msk , , kinesis record aggregation de-aggregation modules lambda . various services offer automatic scaling options keep costs lower provisioning peak volumes . operations monitor kinesis streams msk metrics using cloudwatch . get basic stream topic level metrics addition shard partition level metrics . msk also provides open monitoring prometheus option . kinesis streams developer guide : monitoring kinesis streams managed streaming apache kafka developer guide : monitoring msk cluster plan unexpected / single point failure . components streaming architecture offer different options durability case failure . example , kinesis streams replicates three different availability zones . apache kafka msk , producers configured require acknowledgement partition leader well configurable number in-sync replica followers considering write successful . examples , able plan possible disruptions environment , example , availability zone goes offline , without possible downtime producing consuming layers . security authentication authorization . managed streaming apache kafka developer guide : authentication authorization apache kafka apis kinesis streams developer guide : controlling access kinesis streams resources using iam encryption transit encryption rest . streaming actively moves one layer another , streaming producer stream storage internet private network . protecting transit , enterprises often choose use encrypted connections ( https , ssl , tls ) protect contents transit . many streaming services offer protection rest encryption . well-architected framework security pillar : identity access management lake formation developer guide : security lake formation\"]]\n"
     ]
    }
   ],
   "source": [
    "query = \"For a financial institution processing real-time trading data, we require sub-millisecond latency for transaction processing and strict data immutability for auditing purposes. Which architectural pattern ensures both high performance and non-repudiation?\"\n",
    "response = mistral_first_prompt.generate(query)\n",
    "response_text = response['response']\n",
    "response_context = response['context']\n",
    "print(\"Response: \", response_text)\n",
    "print(\"Context: \", response_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d92db",
   "metadata": {
    "id": "wAtbfZ1WuNQr"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218ca637",
   "metadata": {},
   "source": [
    "### Test Data Set preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db57a4",
   "metadata": {},
   "source": [
    "As mentioned in the previous progress, we will need to create a data set to evaluate the performance of the different models we will be generating as combination of prompt and LLM. To do so we took advantage that the base RAG is of public domain and we can request a more powerfull model to generate those questions and then a human validation will be performed to validate the correctness of the answers.\n",
    "It is important to mention that as part of this initial POC the data is public and the Architectural Patterns beeing used are public patterns, that´s why this is a valid option on this scenario, but when going into using the internal Architectural patterns the questions and answers will need to be generated manually by experts of the different domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d7d26",
   "metadata": {},
   "source": [
    "For this initial project we decided to use Gemini to generate the basic set of questions. The following prompt was used to generate the set of questions.\n",
    "Prompt:\n",
    "\n",
    "*Can you Create 174 Questions about Architectural Decisions based on intent and Non FUnctional Requirments that the expected answer is and Architectural Pattern Decision based on aws only taking into consideration the following siz AWS Well Architected Lenses (Serverless Application, Financial Services Industry, Generative AI, Machine Learning, Migration and Analytics)*\n",
    "\n",
    "\n",
    "\n",
    "*Give me the input in a format I can pass to pandas to use to evaluate an RAG Model I'm creating.*\n",
    "\n",
    "*As well provide from Which lens should the RAG pull the context for the specific Question.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01de9b",
   "metadata": {},
   "source": [
    "The intial set of questions needed to be fix as it did not included the justification, which will be critical for the evaluation as the model's might have different justifications which are valid therefore we need to give that extra contex to the judge model so it can judge propperly if the answer is correct or not.\n",
    "Therefore we executed the following prompt in the same chat context of the Gemin model.\n",
    "\n",
    "*Can you fix the answers to include only the expected services and the justification of each service, no need to include the pattern name.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe70473",
   "metadata": {},
   "source": [
    "With this final prompt we were able to get the right questions, and after a human revition we were able to validate the justififcation and answers were right, and we were able to build our test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c582a",
   "metadata": {},
   "source": [
    "### Pull questions from the Json into a Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0781f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>expected_answer_pattern</th>\n",
       "      <th>relevant_lens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A new e-commerce platform needs to handle high...</td>\n",
       "      <td>[{'name': 'AWS Lambda', 'justification': 'Runs...</td>\n",
       "      <td>Serverless Application</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For a financial institution processing real-ti...</td>\n",
       "      <td>[{'name': 'Amazon Kinesis Data Streams', 'just...</td>\n",
       "      <td>Financial Services Industry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We are building an application for real-time t...</td>\n",
       "      <td>[{'name': 'Amazon API Gateway', 'justification...</td>\n",
       "      <td>Generative AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An anomaly detection system for industrial IoT...</td>\n",
       "      <td>[{'name': 'Amazon Kinesis Data Streams', 'just...</td>\n",
       "      <td>Machine Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A legacy on-premises application with a monoli...</td>\n",
       "      <td>[{'name': 'AWS Application Migration Service (...</td>\n",
       "      <td>Migration</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  A new e-commerce platform needs to handle high...   \n",
       "1  For a financial institution processing real-ti...   \n",
       "2  We are building an application for real-time t...   \n",
       "3  An anomaly detection system for industrial IoT...   \n",
       "4  A legacy on-premises application with a monoli...   \n",
       "\n",
       "                             expected_answer_pattern  \\\n",
       "0  [{'name': 'AWS Lambda', 'justification': 'Runs...   \n",
       "1  [{'name': 'Amazon Kinesis Data Streams', 'just...   \n",
       "2  [{'name': 'Amazon API Gateway', 'justification...   \n",
       "3  [{'name': 'Amazon Kinesis Data Streams', 'just...   \n",
       "4  [{'name': 'AWS Application Migration Service (...   \n",
       "\n",
       "                 relevant_lens  \n",
       "0       Serverless Application  \n",
       "1  Financial Services Industry  \n",
       "2                Generative AI  \n",
       "3             Machine Learning  \n",
       "4                    Migration  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_json_file = \"./questions.json\"\n",
    "questions_df =  pd.read_json(questions_json_file)\n",
    "questions_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11be66f",
   "metadata": {},
   "source": [
    "The questions where imported from Gemini just copying the result into a json file and the json file was then imported to the data frame, the above cell shows the first 5 questions with the relecant data, so we can evaluate the RAG context retrival and the full RAG system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbe6381c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(174, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828af77",
   "metadata": {},
   "source": [
    "In the above cell we can see the data frame pulled the 174 generated questions that will be used for evaluating the different parts of the full model, so we can do adjustments in either the embedding model, the prompt and/or the LLM to get the best results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e5b52",
   "metadata": {},
   "source": [
    "## Judge Model\n",
    "\n",
    "Now that we have a validated set of questions and expected answers, we define our judge model, which will evaluate the outputs of our base model (Mistral) using LLaMA 3.3 70B deployed on Vertex AI.\n",
    "\n",
    "This model acts as an LLM-as-a-judge, a widely adopted technique for evaluating the quality of LLM-generated outputs in the absence of deterministic ground truth. It compares each model-generated response against the expected answer and rates it across three key dimensions:\n",
    "\n",
    "**Technical Accuracy:** Does the answer use correct and contextually appropriate AWS services?\n",
    "\n",
    "**Clarity:** Is the response well-structured, concise, and easy to follow?\n",
    "\n",
    "**Completeness:** Does it address all relevant aspects of the architectural requirement in sufficient detail, without introducing unrelated elements?\n",
    "\n",
    "The evaluation prompt enforces constraints aligned with the AWS Well-Architected Framework and penalizes the use of out-of-context services, deviation from required structure, or lack of configuration detail. The output is returned in a structured JSON format, enabling automated analysis and scoring across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4f4de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class judge_model:\n",
    "    def __init__(self,model_api,model_name):\n",
    "        self.call_vertex_model = call_vertex_model(model_api,model_name)\n",
    "    \n",
    "    def build_judge_prompt(self, question, mistral_response, expected_answer, context):\n",
    "        return f\"\"\"\n",
    "            You are an expert in cloud architecture for financial institutions. Your task is to evaluate a response generated by a language model, given a context, a question (architectural requirement), and an expected answer.\n",
    "\n",
    "            The model was instructed to propose AWS-based solutions that:\n",
    "            - Use **only services found in the provided context**\n",
    "            - Follow the AWS Well-Architected Framework\n",
    "            - Use the following structure: \n",
    "                1. Proposed AWS Architecture\n",
    "                2. Justification\n",
    "                3. AWS Services\n",
    "                4. AWS Only\n",
    "                5. Context Justification\n",
    "            - If the context is insufficient, the model must reply: \"Insufficient context to provide a confident architectural decision\" and explain what's missing.\n",
    "\n",
    "            Below is the evaluation task:\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Architectural Requirement (Question):\n",
    "            {question}\n",
    "\n",
    "            Model-generated answer:\n",
    "            {mistral_response}\n",
    "\n",
    "            Expected answer:\n",
    "            {expected_answer}\n",
    "\n",
    "            Evaluate the model-generated answer using these dimensions (scale 1–5):\n",
    "\n",
    "            - Technical Accuracy\n",
    "            - Clarity\n",
    "            - Completeness\n",
    "\n",
    "            **Important Evaluation Rules**:\n",
    "            - Penalize any use of services not found in the context.\n",
    "            - Penalize if the required structure is not followed.\n",
    "            - Only reward completeness if the answer includes all key components **explicitly relevant to the question**, and provides sufficient detail on how they are used or configured. Including off-topic services or omitting key configuration details should reduce the completeness score.\n",
    "            - Do not reward unnecessary or off-topic information.\n",
    "            - Accuracy should reflect alignment with the Well-Architected Framework and proper AWS service usage.\n",
    "            - Clarity should reflect whether the response is concise, readable, and logically structured.\n",
    "\n",
    "            Provide your evaluation in the following JSON format:\n",
    "\n",
    "            {{\n",
    "            \"accuracy\": <1 to 5>,\n",
    "            \"clarity\": <1 to 5>,\n",
    "            \"completeness\": <1 to 5>,\n",
    "            \"justification\": {{\n",
    "                \"accuracy\": \"Your justification here.\",\n",
    "                \"clarity\": \"Your justification here.\",\n",
    "                \"completeness\": \"Your justification here.\"\n",
    "            }}\n",
    "            }}\n",
    "        \"\"\".strip()\n",
    "\n",
    "    def generate(self, question, base_llm_response, expected_answer, context):\n",
    "        self.base_prompt = self.build_judge_prompt(question, base_llm_response, expected_answer, context)\n",
    "        generated_text = self.call_vertex_model.call_model(self.base_prompt)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d718e05",
   "metadata": {},
   "source": [
    "In this step, we instantiate our judge model using Meta’s LLaMA 3.3 70B Instruct, deployed via Vertex AI’s OpenAPI-compatible endpoint.\n",
    "\n",
    "The judge_model_name specifies the model to be used for evaluation, while judge_model_api defines the full endpoint for accessing the Vertex AI chat interface. We then initialize the judge_model class with these parameters, enabling us to perform structured evaluation of the base model's responses using the custom prompt logic defined earlier.\n",
    "\n",
    "This setup allows our system to decouple the evaluation logic from the underlying model infrastructure, making it modular and easily switchable across different judge LLMs or deployment backends (e.g., Gemini, GPT, Claude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f54c438",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_model_name = \"meta/llama-3.3-70b-instruct-maas\"\n",
    "judge_model_api = \"https://{REGION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{REGION}/endpoints/openapi/chat/completions\"\n",
    "judge_first_prompt = judge_model(judge_model_api,judge_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f2c13",
   "metadata": {},
   "source": [
    "We perform a manual test of the evaluation process by selecting the first question from the dataset. The response is generated using the base model (Mistral) along with its context, and evaluated by the judge model (LLaMA 3.3 70B) against the expected answer. This step is useful to inspect the evaluation logic before applying it across the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7bf05fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS disponible: False\n",
      "CUDA disponible: True\n",
      "Using GPU NVIDIA: NVIDIA GeForce RTX 3070 Ti\n",
      "Total Memory: 8.59 GB\n",
      "Dispositivo activo: cuda\n",
      "Here is the evaluation of the model-generated answer:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"accuracy\": 4,\n",
      "    \"clarity\": 4,\n",
      "    \"completeness\": 3,\n",
      "    \"justification\": {\n",
      "        \"accuracy\": \"The model-generated answer is mostly accurate, as it suggests using AWS Lambda for serverless compute and Amazon EC2 Auto Scaling Groups for predictable workloads, which aligns with the Well-Architected Framework. However, it includes some unnecessary information and services not directly relevant to the question, such as AWS Glue and Redshift.\",\n",
      "        \"clarity\": \"The response is generally clear and well-structured, but it is lengthy and includes some off-topic information, which reduces its overall clarity. The use of headings and bullet points helps to organize the content, but some sections are not directly relevant to the question.\",\n",
      "        \"completeness\": \"The answer is partially complete, as it addresses the need for automatic scaling and cost optimization, but it does not explicitly mention Amazon API Gateway, which is a key component for handling API requests and routing them to Lambda. Additionally, some details on how the services are configured or used are missing, such as how Lambda is integrated with API Gateway or how Auto Scaling is configured.\"\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Note that I penalized the completeness score because the answer does not explicitly mention Amazon API Gateway, which is a crucial component for handling API requests and routing them to Lambda. I also reduced the clarity score due to the inclusion of off-topic information and the length of the response. The accuracy score is relatively high, as the answer aligns with the Well-Architected Framework and suggests relevant AWS services, but I deducted a point for the inclusion of unnecessary information.\n"
     ]
    }
   ],
   "source": [
    "test_question = questions_df['question'].iloc[0]\n",
    "test_expected_answer = questions_df['expected_answer_pattern'].iloc[0]\n",
    "test_mistral_response = mistral_first_prompt.generate(questions_df['question'].iloc[0])\n",
    "test_mistral_response_text = test_mistral_response['response']\n",
    "test_mistral_response_context = test_mistral_response['context']\n",
    "\n",
    "response = judge_first_prompt.generate(test_question, test_mistral_response, test_expected_answer, test_mistral_response_context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b45e74",
   "metadata": {},
   "source": [
    "This function extracts and parses the JSON evaluation returned by the judge model. It looks for a JSON block enclosed in triple backticks (```) and converts it into a Python dictionary. If no valid JSON is found or parsing fails, it returns an empty dictionary and prints the error. This step ensures structured access to the evaluation scores and justifications for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6785f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judge_data(judge_text):\n",
    "    try:\n",
    "        match = re.search(r\"```([\\s\\S]*?)```\", judge_text)\n",
    "        if match:\n",
    "            json_str = match.group(1).strip()\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            print(\"No JSON block found between triple backticks.\")\n",
    "            return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON decode error: {e}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected parsing error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd7d9d4",
   "metadata": {},
   "source": [
    "This function determines whether a model response is considered acceptable based on evaluation scores. It checks if the average score meets a minimum threshold (default: 4.0) and that each individual score (accuracy, clarity, and completeness) is at least a given minimum (default: 3). The result is a Boolean value indicating whether the response passes the defined quality criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8abb71b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_acceptable(row, threshold=4.0, min_each=3):\n",
    "    scores = [row['accuracy_score'], row['clarity_score'], row['completeness_score']]\n",
    "    return row['avg_score'] >= threshold and all(score >= min_each for score in scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "60965c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = questions_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8442bf52",
   "metadata": {},
   "source": [
    "Next, this block performs a batch evaluation loop over the dataset of questions using a base model (Mistral) and a judge model (LLaMA 3.3 70B). For each row in the dataset:\n",
    "\n",
    "1.- The base model generates a response and context based on the input question.\n",
    "\n",
    "2.- The judge model evaluates this response against the expected answer using a structured scoring prompt.\n",
    "\n",
    "3.- The evaluation result is parsed to extract scores (accuracy, clarity, completeness) and their justifications.\n",
    "\n",
    "4.- An average score is computed, and a Boolean flag (is_correct) is assigned based on predefined quality thresholds.\n",
    "\n",
    "This process populates the eval_df with all relevant outputs and metrics, enabling downstream analysis of model performance at both individual and aggregate levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "eval_df['base_model_response'] = \"\"\n",
    "eval_df['judge_model_eval'] = \"\"\n",
    "eval_df['accuracy_score'] = 0\n",
    "eval_df['clarity_score'] = 0\n",
    "eval_df['completeness_score'] = 0\n",
    "eval_df['accuracy_just'] = \"\"\n",
    "eval_df['clarity_just'] = \"\"\n",
    "eval_df['completeness_just'] = \"\"\n",
    "eval_df['avg_score'] = 0.0\n",
    "eval_df['is_correct'] = False\n",
    "\n",
    "for idx, row in tqdm(eval_df.iterrows(), total=eval_df.shape[0]):\n",
    "    question = row['question']\n",
    "    expected = row['expected_answer_pattern']\n",
    "\n",
    "    base_model_output = mistral_first_prompt.generate(question)\n",
    "    eval_df.at[idx, 'base_model_response'] = base_model_output['response']\n",
    "\n",
    "    evaluation_text = judge_first_prompt.generate(question, base_model_output['response'], expected, base_model_output['context'])\n",
    "    eval_df.at[idx, 'judge_model_eval'] = evaluation_text\n",
    "\n",
    "    eval_dict = extract_judge_data(evaluation_text)\n",
    "\n",
    "    eval_df.at[idx, 'accuracy_score'] = eval_dict.get('accuracy', 0)\n",
    "    eval_df.at[idx, 'clarity_score'] = eval_dict.get('clarity', 0)\n",
    "    eval_df.at[idx, 'completeness_score'] = eval_dict.get('completeness', 0)\n",
    "    eval_df.at[idx, 'accuracy_just'] = eval_dict.get('justification', {}).get('accuracy', '')\n",
    "    eval_df.at[idx, 'clarity_just'] = eval_dict.get('justification', {}).get('clarity', '')\n",
    "    eval_df.at[idx, 'completeness_just'] = eval_dict.get('justification', {}).get('completeness', '')\n",
    "\n",
    "    avg = (eval_dict.get('accuracy', 0) +\n",
    "           eval_dict.get('clarity', 0) +\n",
    "           eval_dict.get('completeness', 0)) / 3.0\n",
    "    eval_df.at[idx, 'avg_score'] = avg\n",
    "\n",
    "    eval_df.at[idx, 'is_correct'] = is_acceptable(eval_df.loc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3241c4ad",
   "metadata": {},
   "source": [
    "This snippet calculates the model accuracy over a subset of the dataset (rows 0 to 51). It counts how many responses were marked as is_correct = True and divides that by the total number of samples in the slice. The result is printed as a percentage, reflecting how well the model performed within this evaluation window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1eb5abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy over 52 questions: 59.62%\n"
     ]
    }
   ],
   "source": [
    "total = len(eval_df.iloc[0:52])\n",
    "correct = eval_df.iloc[0:52]['is_correct'].sum()\n",
    "model_accuracy = correct / total\n",
    "\n",
    "print(f\"Model Accuracy over {total} questions: {model_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40c474",
   "metadata": {},
   "source": [
    "The model achieved an accuracy of 59.62% over the evaluated subset of 52 questions. This means that slightly more than half of the generated responses met the predefined quality criteria across technical accuracy, clarity, and completeness. While this indicates a baseline level of competence, there is still significant room for improvement—particularly in generating more focused, precise, and context-aligned answers. The results suggest that the current base model (Mistral) can serve as a starting point, but further refinement, prompt engineering, or model selection may be necessary to reach production-level reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d010d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo guardado exitosamente como 'subset_0_to_52.xlsx'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    eval_df.to_excel('subset_0_to_52.xlsx', sheet_name='Evaluation', index=False)\n",
    "    print(f\"✅ Archivo guardado exitosamente como 'subset_0_to_52.xlsx'\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al guardar archivo: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
