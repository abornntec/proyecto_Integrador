[
    {
        "question": "A new e-commerce platform needs to handle highly variable traffic, scaling from zero to millions of requests per day, with an emphasis on cost optimization and automatic scaling. How would you design the compute layer to minimize idle resources and automatically adjust to demand spikes?",
        "expected_answer_pattern": [
            {"name": "AWS Lambda", "justification": "Runs code without provisioning or managing servers, automatically scales with demand, and you only pay for compute time consumed, ideal for highly variable traffic and cost optimization."},
            {"name": "Amazon API Gateway", "justification": "Acts as the 'front door' for the application, handling API requests, routing them to Lambda, and managing traffic, scaling automatically to millions of requests."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a financial institution processing real-time trading data, we require sub-millisecond latency for transaction processing and strict data immutability for auditing purposes. Which architectural pattern ensures both high performance and non-repudiation?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Provides real-time data ingestion and processing with high throughput and low latency, essential for sub-millisecond trading data."},
            {"name": "AWS Lambda", "justification": "Processes Kinesis stream records in near real-time with low latency, executing code without server management."},
            {"name": "Amazon DynamoDB", "justification": "Offers single-digit millisecond performance at any scale for transaction processing, and its streams can be used for auditing, supporting immutability when combined with versioning or ledger techniques."},
            {"name": "Amazon QLDB (Quantum Ledger Database)", "justification": "Specifically designed for immutable, cryptographically verifiable transaction logs, ensuring data non-repudiation for auditing."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "We are building an application for real-time text summarization using a large language model. The solution must be highly scalable, cost-efficient for inference, and support rapid iteration of different model versions. What architectural pattern would you recommend?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Provides a scalable HTTP endpoint for user requests, routing them to the inference logic."},
            {"name": "AWS Lambda", "justification": "Serves as the orchestrator and business logic layer, handling the summarization request and invoking the SageMaker endpoint."},
            {"name": "Amazon SageMaker Serverless Inference", "justification": "Deploys LLMs for inference without managing servers, automatically scales from zero to peak demand, and you pay only for inference duration and data processed, making it cost-efficient for variable loads and supporting rapid model iteration."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "An anomaly detection system for industrial IoT sensors needs to process terabytes of streaming data daily. The solution must provide near real-time insights and be capable of retraining models frequently with new data. What architectural pattern supports this continuous learning and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests high-volume streaming data from IoT sensors in real-time."},
            {"name": "Amazon Kinesis Data Analytics", "justification": "Processes and analyzes streaming data in near real-time using SQL or Apache Flink for immediate anomaly detection."},
            {"name": "Amazon SageMaker", "justification": "Used for building, training, and deploying ML models for anomaly detection, supporting frequent retraining with new data."},
            {"name": "Amazon S3", "justification": "Acts as the central data lake for storing raw and processed streaming data for model retraining and historical analysis."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "A legacy on-premises application with a monolithic architecture needs to be migrated to AWS. The primary goal is to re-host the application quickly with minimal code changes, while setting the stage for future modernization. What migration strategy and associated pattern would you apply?",
        "expected_answer_pattern": [
            {"name": "AWS Application Migration Service (AWS MGN)", "justification": "Automates the re-hosting of physical or virtual servers to AWS, minimizing downtime and code changes for a quick migration."},
            {"name": "Amazon EC2", "justification": "Provides compute capacity (virtual servers) in the cloud to host the migrated application with minimal changes to its operating system or dependencies."},
            {"name": "Amazon EBS", "justification": "Provides block storage for EC2 instances, replacing on-premises disks with durable and scalable cloud storage."},
            {"name": "Amazon VPC", "justification": "Creates an isolated virtual network in AWS, allowing the migrated application to operate in a familiar network environment."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A retail company wants to analyze historical sales data from various sources (CRM, ERP, e-commerce) to identify customer purchasing trends. The solution must handle diverse data formats, support ad-hoc querying, and scale to petabytes of data for long-term retention. What data lake pattern would be most suitable?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Provides highly scalable, durable, and cost-effective object storage for raw and processed data, supporting diverse data formats and petabytes of data for long-term retention."},
            {"name": "AWS Glue", "justification": "A serverless data integration service for ETL (Extract, Transform, Load) operations, allowing discovery, transformation, and preparation of data from various sources for analysis, handling diverse data formats."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that makes it easy to analyze data directly in S3 using standard SQL, ideal for ad-hoc querying without managing infrastructure."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a backend for a mobile application that requires instant response times, highly available user authentication, and automatic scaling for millions of users without managing servers. Which serverless pattern best fits these requirements?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Provides a highly scalable and secure API endpoint, handling authentication and routing requests from mobile clients."},
            {"name": "AWS Lambda", "justification": "Executes backend logic in a serverless manner, processing requests with instant response times and scaling automatically based on demand."},
            {"name": "Amazon DynamoDB", "justification": "A fully managed NoSQL database offering single-digit millisecond performance at any scale, suitable for storing application data and user profiles with high availability."},
            {"name": "Amazon Cognito", "justification": "Manages user authentication and authorization for millions of users, providing highly available user identity management without managing servers."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a regulated financial workload, data residency in a specific region is a strict compliance requirement, and all data access must be highly secured and auditable. Which architectural pattern ensures geographic data control and robust access logging?",
        "expected_answer_pattern": [
            {"name": "Amazon VPC", "justification": "Creates an isolated virtual network within the specified AWS region, ensuring data traffic remains within that geographic boundary."},
            {"name": "Amazon S3", "justification": "Stores data within the designated region, with bucket policies and encryption to enforce residency. S3 Object Lock can enforce immutability for compliance."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages encryption keys for data at rest within the specified region, ensuring cryptographic control over sensitive data."},
            {"name": "AWS CloudTrail", "justification": "Provides comprehensive logging of all API calls and actions taken within the AWS account, enabling robust auditing of data access and changes for compliance."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI model will be used for creating marketing content. The inference process is computationally intensive, but not constant, requiring bursting capabilities. How can we ensure cost-effective inference without sacrificing responsiveness for peak demand?",
        "expected_answer_pattern": [
            {"name": "Amazon SQS", "justification": "Decouples the request submission from the inference execution, acting as a buffer for incoming requests and allowing for asynchronous processing and bursting."},
            {"name": "AWS Lambda", "justification": "Acts as the consumer of messages from SQS, triggering the inference process. It scales automatically to handle bursts of requests without managing servers."},
            {"name": "Amazon SageMaker Serverless Inference", "justification": "Deploys the computationally intensive generative AI model. It automatically scales from zero to hundreds of concurrent invocations, providing bursting capabilities while optimizing costs by only charging for active inference time."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our machine learning pipeline needs to process batch jobs for large datasets (up to terabytes) daily. The processing needs to be cost-effective and handle failures gracefully, resuming from the point of failure. Which architectural pattern is optimized for reliable, scalable batch processing?",
        "expected_answer_pattern": [
            {"name": "AWS Step Functions", "justification": "Orchestrates the entire batch processing workflow, allowing for sequential or parallel execution of tasks, handling retries, and managing state to resume from failure points."},
            {"name": "AWS Batch", "justification": "Dynamically provisions and manages compute resources (EC2 instances or Fargate) for running containerized batch jobs, optimizing for cost and scalability by using managed queues and compute environments."},
            {"name": "Amazon S3", "justification": "Serves as the durable and scalable storage for input datasets (terabytes) and output results of the batch jobs, ensuring data is available and persistent."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An on-premises data center migration requires minimizing downtime for critical applications. We need a strategy that allows us to cut over traffic to AWS gradually, testing each component before full transition. Which migration pattern supports this phased approach?",
        "expected_answer_pattern": [
            {"name": "AWS Database Migration Service (DMS)", "justification": "Enables continuous data replication from on-premises databases to AWS, minimizing downtime during the database cutover and allowing for phased testing."},
            {"name": "Amazon RDS (or Amazon DynamoDB/Aurora)", "justification": "Provides a fully managed database service on AWS for the migrated database, allowing for a gradual cutover of application traffic to the cloud database."},
            {"name": "AWS Application Migration Service (AWS MGN)", "justification": "Automates the replication of source servers to AWS, providing a consistent copy of the application for cutover testing in stages."},
            {"name": "Amazon Route 53", "justification": "Manages DNS routing, allowing for a controlled, gradual redirection of traffic from on-premises to AWS endpoints during cutover, enabling phased testing."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A gaming company collects vast amounts of player activity data for real-time analytics dashboards. The data arrives in bursts, and the dashboards need to update within seconds. What real-time analytics pattern would support this?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests high-volume, bursty player activity data in real-time, providing a durable buffer for downstream processing."},
            {"name": "AWS Lambda", "justification": "Processes Kinesis records in near real-time, performing transformations or aggregations before storing data for dashboards."},
            {"name": "Amazon OpenSearch Service (formerly Elasticsearch Service)", "justification": "Provides a scalable, distributed search and analytics engine suitable for real-time indexing and querying of player activity data, powering dashboards with sub-second latency."},
            {"name": "Amazon QuickSight", "justification": "A cloud-native business intelligence service that can directly connect to OpenSearch Service to create and update real-time dashboards."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a new application that needs to be resilient to regional outages and maintain continuous availability. How would you design a serverless architecture to provide multi-region disaster recovery and high availability?",
        "expected_answer_pattern": [
            {"name": "Amazon Route 53", "justification": "Provides global DNS with routing policies (e.g., latency-based, failover) to direct user traffic to the healthy, closest, or primary region."},
            {"name": "Amazon DynamoDB Global Tables", "justification": "Offers multi-region, active-active replication for DynamoDB tables, ensuring low-latency reads and writes and data consistency across regions."},
            {"name": "AWS Lambda", "justification": "Deploys the application logic in multiple regions, ensuring compute capacity is available even if one region experiences an outage. For Active-Active, Lambda functions can directly access global tables. For Active-Passive, Lambda in the standby region is ready to activate."},
            {"name": "Amazon S3 Cross-Region Replication", "justification": "Replicates static content or data stored in S3 buckets across regions for disaster recovery and content availability."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a banking application, all customer interactions must be secured end-to-end with strong encryption in transit and at rest, including compliance with industry standards like PCI DSS. Which security pattern ensures this level of protection?",
        "expected_answer_pattern": [
            {"name": "AWS Key Management Service (KMS)", "justification": "Centrally manages and controls encryption keys used for data at rest across various AWS services, ensuring strong cryptographic controls and compliance requirements like PCI DSS."},
            {"name": "AWS Certificate Manager (ACM)", "justification": "Manages SSL/TLS certificates used to encrypt data in transit (e.g., for API Gateway, Load Balancers), ensuring secure communication channels."},
            {"name": "Amazon VPC", "justification": "Provides a logically isolated section of the AWS Cloud, allowing for private network configurations to minimize exposure and enforce network-level security."},
            {"name": "Amazon S3", "justification": "Stores data at rest with Server-Side Encryption using KMS, supporting strong encryption and compliance needs."},
            {"name": "Amazon RDS", "justification": "Provides encryption at rest for databases using KMS, ensuring sensitive data is protected."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "We need to create a recommendation engine that dynamically generates personalized content using a large language model. The solution must provide low-latency inference and be easily integrated into existing applications. What architectural pattern supports this use case?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Provides a low-latency, scalable HTTP endpoint for applications to request personalized content."},
            {"name": "AWS Lambda", "justification": "Acts as the immediate compute layer, processing the request, enriching context (e.g., from DynamoDB), and invoking the SageMaker endpoint for content generation."},
            {"name": "Amazon SageMaker", "justification": "Hosts the large language model as a real-time endpoint for low-latency inference, generating personalized content based on the input."},
            {"name": "Amazon DynamoDB", "justification": "Stores user profiles or other context necessary for personalization, providing fast lookup for Lambda functions."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML model serving needs to support A/B testing of different model versions to compare performance and user impact. How can we deploy and manage multiple model versions simultaneously for controlled experimentation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Endpoints with Production Variants", "justification": "Allows deploying multiple model versions (variants) behind a single endpoint, enabling routing of a specified percentage of traffic to each variant for A/B testing."},
            {"name": "Amazon API Gateway", "justification": "Provides the external interface for applications to call the ML model, routing requests to the SageMaker endpoint."},
            {"name": "AWS Lambda", "justification": "Can be used as a proxy or orchestrator between API Gateway and SageMaker, adding logic for custom routing, logging, or metric collection for A/B test analysis."},
            {"name": "Amazon CloudWatch", "justification": "Monitors metrics from SageMaker endpoints and Lambda, allowing for real-time observation and comparison of performance, latency, and error rates between model variants."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An existing application is a legacy mainframe system that needs to be re-platformed to AWS. The goal is to move the application without major redesign but modernize the underlying database. What migration approach would you take?",
        "expected_answer_pattern": [
            {"name": "AWS Application Migration Service (AWS MGN)", "justification": "Used for re-hosting the mainframe application components (if applicable, e.g., surrounding distributed systems) to EC2 with minimal changes."},
            {"name": "AWS Database Migration Service (DMS)", "justification": "Migrates the mainframe's legacy database to a modern AWS managed database service, handling schema conversion and continuous data replication."},
            {"name": "Amazon RDS (or Amazon Aurora)", "justification": "Provides a fully managed, scalable, and highly available relational database service on AWS, modernizing the database layer while reducing operational overhead."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A supply chain company needs to consolidate data from various suppliers' systems for comprehensive reporting. Data volume is high, and reports are run daily. The solution must support complex SQL queries over the entire dataset. What data warehousing pattern would be appropriate?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Acts as the landing zone and cost-effective storage for raw and transformed data before loading into the data warehouse."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that discovers, transforms, and loads data from S3 into Amazon Redshift, handling diverse data formats and preparing data for reporting."},
            {"name": "Amazon Redshift", "justification": "A fully managed, petabyte-scale cloud data warehouse optimized for complex SQL queries over large datasets, providing high performance for daily reporting needs."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless REST API that can manage millions of concurrent connections and handle authentication for mobile users. Which pattern provides robust API management and scaling for this scenario?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Manages API requests, supports millions of concurrent connections, and integrates directly with AWS Lambda for backend logic."},
            {"name": "AWS Lambda", "justification": "Executes the backend business logic in a serverless manner, scaling automatically to handle high demand."},
            {"name": "Amazon Cognito", "justification": "Provides robust user authentication and identity management for mobile users, integrating seamlessly with API Gateway to secure API access."},
            {"name": "Amazon DynamoDB", "justification": "Offers a highly scalable, low-latency NoSQL database for storing application data and user profiles, supporting millions of concurrent users."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a credit card fraud detection system, regulatory requirements dictate that all data must be retained for 7 years in an immutable and tamper-proof manner. Which storage pattern provides long-term, compliant data archiving?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost storage for long-term archiving of data (7 years), specifically designed for data that is rarely accessed."},
            {"name": "Amazon S3 Object Lock", "justification": "Enables Write Once Read Many (WORM) capability for objects stored in S3, ensuring immutability and tamper-proofing to meet regulatory compliance requirements for data retention."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts data stored in S3 Glacier Deep Archive, providing an additional layer of security and control over sensitive data."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "We are developing a new application that leverages a foundational model for content generation. The output needs to be reviewed and potentially refined by human operators before final publication. Which architectural pattern facilitates human-in-the-loop validation for generative AI?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Ground Truth", "justification": "Used for human-in-the-loop validation of generated content, allowing human operators to review, refine, and provide feedback on the AI's output."},
            {"name": "Amazon SQS", "justification": "Queues the generated content for human review, decoupling the generation process from the validation workflow and ensuring reliable delivery of review tasks."},
            {"name": "AWS Lambda", "justification": "Orchestrates the workflow, triggering content generation, pushing content to SQS for review, and processing human feedback for model improvement."},
            {"name": "Amazon S3", "justification": "Stores the generated content and associated metadata for human review and final publication."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML pipeline needs to automate the entire process from data ingestion to model deployment, including automated retraining. Which architectural pattern supports continuous integration and continuous delivery for machine learning?",
        "expected_answer_pattern": [
            {"name": "AWS CodePipeline", "justification": "Orchestrates the end-to-end MLOps pipeline, automating the stages from data ingestion to model deployment."},
            {"name": "AWS CodeBuild", "justification": "Compiles code and runs tests for ML artifacts (e.g., model code, inference scripts)."},
            {"name": "Amazon SageMaker Pipelines", "justification": "A purpose-built MLOps service within SageMaker to create, manage, and orchestrate ML workflows, including automated retraining and model deployment steps."},
            {"name": "Amazon S3", "justification": "Stores source data, model artifacts, and processed data throughout the MLOps pipeline."},
            {"name": "AWS Step Functions", "justification": "Can orchestrate complex, multi-step ML workflows, including conditional logic, error handling, and parallel processing for tasks like data preprocessing or model training."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise has a complex, interconnected set of applications on-premises. They need to migrate to AWS in a phased approach, minimizing disruption and ensuring interoperability during the transition. What migration pattern supports a hybrid state?",
        "expected_answer_pattern": [
            {"name": "AWS Direct Connect", "justification": "Provides a dedicated, high-bandwidth network connection between on-premises data centers and AWS, ensuring reliable and low-latency hybrid connectivity for interoperability during transition."},
            {"name": "AWS Site-to-Site VPN", "justification": "Establishes a secure IPsec tunnel between on-premises networks and AWS VPCs over the public internet, suitable for hybrid cloud connectivity."},
            {"name": "AWS Transit Gateway", "justification": "Acts as a central hub to connect VPCs and on-premises networks, simplifying network routing and management across a hybrid cloud architecture."},
            {"name": "Amazon Route 53", "justification": "Manages DNS resolution for applications running in both on-premises and AWS, allowing for a phased cutover of traffic and ensuring interoperability."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A large manufacturing company wants to analyze equipment telemetry data in near real-time to predict failures. The data volume is high-velocity and continuous. What analytics pattern is best for processing and querying streaming data for operational insights?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests high-velocity, continuous telemetry data from manufacturing equipment."},
            {"name": "Amazon Kinesis Data Analytics", "justification": "Processes streaming data in near real-time using SQL or Apache Flink, enabling immediate anomaly detection and operational insights."},
            {"name": "Amazon DynamoDB", "justification": "Stores processed real-time data for low-latency queries and operational dashboards, providing high throughput and consistent performance."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Visualizes real-time insights for operational monitoring dashboards."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build an IoT data ingestion pipeline that can handle millions of messages per second from devices globally, ensuring durability and low latency for data delivery to downstream services. Which serverless pattern is designed for high-throughput messaging?",
        "expected_answer_pattern": [
            {"name": "AWS IoT Core", "justification": "Manages connections with millions of IoT devices, securely ingesting messages at high scale."},
            {"name": "Amazon Kinesis Data Streams", "justification": "Acts as a highly scalable and durable buffer for ingesting IoT messages, ensuring low-latency delivery to downstream services even with millions of messages per second."},
            {"name": "AWS Lambda", "justification": "Processes messages from Kinesis Data Streams, performing light transformations or routing to other services without managing servers."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a payment processing system, we require guaranteed transactionality and consistency for financial ledger entries, even during high concurrency. Which AWS database service is best suited to meet these strict ACID requirements?",
        "expected_answer_pattern": [
            {"name": "Amazon RDS for PostgreSQL (or MySQL, SQL Server, Oracle)", "justification": "Provides a fully managed relational database service supporting strong ACID (Atomicity, Consistency, Isolation, Durability) properties, essential for financial ledger entries and guaranteed transactionality."},
            {"name": "Amazon Aurora (PostgreSQL-compatible or MySQL-compatible)", "justification": "A highly performant and scalable relational database service compatible with PostgreSQL and MySQL, offering enterprise-grade performance and durability for high-concurrency payment processing."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A new product aims to allow users to generate custom images based on text prompts. The image generation process can take several seconds to minutes. How can we provide an asynchronous user experience while handling the long-running generative AI tasks efficiently?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Receives user requests and immediately returns an acknowledgment, allowing for an asynchronous user experience."},
            {"name": "Amazon SQS", "justification": "Buffers image generation requests received from API Gateway, ensuring durability and decoupling the request submission from the long-running generation process."},
            {"name": "AWS Lambda", "justification": "Triggers the image generation task on SageMaker in response to SQS messages, and can update the user on progress or results (e.g., via SQS callback or WebSockets)."},
            {"name": "Amazon SageMaker", "justification": "Hosts the generative AI model for image creation. It can be configured with asynchronous endpoints or batch transform jobs to handle long-running tasks efficiently without blocking user interaction."},
            {"name": "Amazon S3", "justification": "Stores the generated images, making them accessible to users once completed."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML model training requires specialized hardware (GPUs) and needs to be orchestrated as part of a larger workflow, with automatic scaling for training jobs and robust logging. Which pattern supports managed, distributed ML training?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Training Jobs", "justification": "Provides a fully managed service for training ML models, allowing specification of instance types with GPUs, handling distributed training, and integrating with other AWS services."},
            {"name": "Amazon S3", "justification": "Stores large datasets for training and model artifacts, ensuring data durability and accessibility for SageMaker training jobs."},
            {"name": "AWS Step Functions", "justification": "Orchestrates complex ML workflows, including data preparation, training job execution, and model evaluation, providing robust logging and state management."},
            {"name": "Amazon CloudWatch", "justification": "Collects logs and metrics from SageMaker training jobs and other services, enabling robust monitoring and debugging."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise has a large portfolio of applications that need to be moved to AWS. They require a centralized dashboard to track the progress of all migrations and manage resources across different migration waves. What migration governance pattern is required?",
        "expected_answer_pattern": [
            {"name": "AWS Migration Hub", "justification": "Provides a centralized dashboard to track the progress of application migrations across various AWS and partner migration tools, enabling visibility and management for multiple migration waves."},
            {"name": "AWS Application Discovery Service", "justification": "Collects data about on-premises servers and applications, populating Migration Hub with information needed for planning and tracking."},
            {"name": "AWS Control Tower (or AWS Organizations)", "justification": "Establishes a well-architected multi-account environment, providing a framework for managing resources and governance across projects during migration."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A data science team needs a secure, isolated environment to perform ad-hoc data exploration and develop new analytical models on sensitive data. What analytics pattern provides a sandboxed environment for data experimentation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Studio", "justification": "Provides a web-based integrated development environment (IDE) for ML, supporting interactive data exploration with notebooks and direct access to data sources securely."},
            {"name": "AWS Lake Formation", "justification": "Defines fine-grained access controls on data stored in S3, ensuring that sensitive data is only accessible to authorized users and services within the sandbox environment."},
            {"name": "Amazon S3", "justification": "Serves as the secure data lake for sensitive data, with encryption and bucket policies to enforce isolation and access restrictions."},
            {"name": "Amazon VPC", "justification": "Provides network isolation for SageMaker Studio notebooks and data processing resources, ensuring that sensitive data remains within a private network."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless cron job that runs daily to process data and send notifications. The job must be highly reliable and automatically retry on failure. Which serverless pattern ensures reliable scheduled execution?",
        "expected_answer_pattern": [
            {"name": "Amazon EventBridge (formerly CloudWatch Events)", "justification": "Provides a serverless event bus that can be configured with scheduled rules (cron expressions) to trigger daily jobs."},
            {"name": "AWS Lambda", "justification": "Executes the data processing and notification logic in a serverless manner, triggered by EventBridge. Lambda functions automatically retry on certain failures for increased reliability."},
            {"name": "AWS Step Functions", "justification": "Orchestrates complex multi-step workflows for the cron job, handling retries, error handling, and state management, ensuring high reliability for multi-stage processes."},
            {"name": "Amazon SNS", "justification": "Sends notifications (e.g., email, SMS) once the data processing is complete or if errors occur."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a banking application, all sensitive customer data must be encrypted at rest with customer-managed keys (CMKs) and all access to these keys must be logged and auditable. Which security pattern ensures CMK management and logging?",
        "expected_answer_pattern": [
            {"name": "AWS Key Management Service (KMS)", "justification": "Provides a fully managed service for creating and controlling encryption keys (CMKs), enabling encryption of data at rest with strong cryptographic controls."},
            {"name": "AWS CloudTrail", "justification": "Records all API calls made to KMS, logging every access and use of CMKs, ensuring auditable key management for compliance."},
            {"name": "Amazon S3", "justification": "Stores sensitive customer data encrypted at rest using KMS CMKs."},
            {"name": "Amazon RDS", "justification": "Encrypts database data at rest using KMS CMKs, ensuring sensitive data in databases is protected."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "We are developing an application that leverages a foundational model for content generation. The output needs to be reviewed and potentially refined by human operators before final publication. Which architectural pattern facilitates human-in-the-loop validation for generative AI?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Ground Truth", "justification": "Used for human-in-the-loop validation of generated content, allowing human operators to review, refine, and provide feedback on the AI's output."},
            {"name": "Amazon SQS", "justification": "Queues the generated content for human review, decoupling the generation process from the validation workflow and ensuring reliable delivery of review tasks."},
            {"name": "AWS Lambda", "justification": "Orchestrates the workflow, triggering content generation, pushing content to SQS for review, and processing human feedback for model improvement."},
            {"name": "Amazon S3", "justification": "Stores the generated content and associated metadata for human review and final publication."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML pipeline needs to automate data labeling and annotation for large image datasets to improve model accuracy. Which pattern facilitates efficient data labeling for machine learning models?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Ground Truth", "justification": "Provides a managed service for building high-quality training datasets for machine learning by enabling human annotators or automated labeling to label image data efficiently."},
            {"name": "Amazon S3", "justification": "Stores the raw image datasets and the completed labeled datasets, serving as the central repository for labeling inputs and outputs."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An on-premises application relies heavily on shared file storage. We need to migrate this storage to AWS while maintaining file system semantics for the application. Which migration pattern addresses file storage migration?",
        "expected_answer_pattern": [
            {"name": "AWS DataSync", "justification": "Automates and accelerates the migration of large amounts of file data from on-premises to AWS, while preserving file system metadata and permissions."},
            {"name": "Amazon FSx for Windows File Server", "justification": "Provides a fully managed native Microsoft Windows file system, maintaining file system semantics (SMB protocol) for applications that require it."},
            {"name": "Amazon FSx for Lustre", "justification": "Provides a high-performance file system optimized for compute-intensive workloads like HPC, often used for migrating POSIX-compliant file systems with high-throughput requirements."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A data engineering team needs to run complex ETL jobs on a data lake, transforming raw data into refined formats for analytics. The jobs are resource-intensive and require serverless processing. What serverless ETL pattern is suitable?",
        "expected_answer_pattern": [
            {"name": "AWS Glue", "justification": "A serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, providing a managed Apache Spark environment for resource-intensive ETL jobs without managing servers."},
            {"name": "Amazon S3", "justification": "Serves as the central data lake, storing raw data and the refined output of ETL jobs in a scalable and cost-effective manner."},
            {"name": "AWS Lake Formation", "justification": "Simplifies security management for data lakes, allowing fine-grained access control to data within S3, which is processed by AWS Glue."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless API that is highly secure, allowing access only from specific IP ranges or VPCs, and enforcing fine-grained authorization. Which serverless pattern ensures network-level security and authorization for APIs?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway Private Endpoints", "justification": "Enables creation of private API endpoints accessible only from within your Amazon VPC or on-premises networks via Direct Connect/VPN, ensuring network-level security."},
            {"name": "AWS Lambda", "justification": "Executes the backend logic, triggered by API Gateway, operating within a VPC to access private resources and maintain network isolation."},
            {"name": "Amazon VPC", "justification": "Provides network isolation for API Gateway private endpoints and Lambda functions, allowing control over IP ranges and connectivity."},
            {"name": "AWS Identity and Access Management (IAM)", "justification": "Enforces fine-grained authorization for API access, controlling which users or roles can invoke specific API methods."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a real-time fraud detection system in financial services, we need to quickly query large, semi-structured datasets of user behavior. The solution must support rapid schema evolution and high read/write throughput. Which database pattern is suitable?",
        "expected_answer_pattern": [
            {"name": "Amazon DynamoDB", "justification": "A fully managed NoSQL key-value and document database offering single-digit millisecond performance at any scale, supporting high read/write throughput and rapid schema evolution for semi-structured user behavior data."},
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests real-time user behavior data for near-immediate processing and storage into DynamoDB."},
            {"name": "AWS Lambda", "justification": "Processes data from Kinesis Data Streams and writes to DynamoDB for real-time querying."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "We are developing an intelligent chatbot that leverages a generative AI model. The chatbot needs to maintain conversational context over multiple turns and provide relevant, consistent responses. What architectural pattern supports conversational AI with LLMs?",
        "expected_answer_pattern": [
            {"name": "Amazon Lex", "justification": "Provides a managed service for building conversational interfaces, handling natural language understanding and dialogue management, and integrating with backend services."},
            {"name": "Amazon Bedrock (or Amazon SageMaker for custom models)", "justification": "Provides access to foundational models (LLMs) for generating human-like text responses based on conversational context."},
            {"name": "AWS Lambda", "justification": "Acts as the fulfillment logic for Lex intents, orchestrating interactions with Bedrock/SageMaker and managing conversational state."},
            {"name": "Amazon DynamoDB", "justification": "Stores conversational context and session data for the chatbot, enabling multi-turn conversations and consistent responses."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML pipeline needs to automate the entire process from data ingestion to model deployment, including automated retraining. Which architectural pattern supports continuous integration and continuous delivery for machine learning?",
        "expected_answer_pattern": [
            {"name": "AWS CodePipeline", "justification": "Orchestrates the end-to-end MLOps pipeline, automating the stages from data ingestion to model deployment."},
            {"name": "AWS CodeBuild", "justification": "Compiles code and runs tests for ML artifacts (e.g., model code, inference scripts)."},
            {"name": "Amazon SageMaker Pipelines", "justification": "A purpose-built MLOps service within SageMaker to create, manage, and orchestrate ML workflows, including automated retraining and model deployment steps."},
            {"name": "Amazon S3", "justification": "Stores source data, model artifacts, and processed data throughout the MLOps pipeline."},
            {"name": "AWS Step Functions", "justification": "Can orchestrate complex, multi-step ML workflows, including conditional logic, error handling, and parallel processing for tasks like data preprocessing or model training."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise has a complex, interconnected set of applications on-premises that need to be re-architected into microservices as part of the migration to AWS. What migration strategy would be most suitable for this transformation?",
        "expected_answer_pattern": [
            {"name": "Amazon Elastic Kubernetes Service (EKS) or Amazon Elastic Container Service (ECS)", "justification": "Provides managed container orchestration platforms, enabling re-architecting monolithic applications into microservices and deploying them as containers."},
            {"name": "AWS Fargate", "justification": "A serverless compute engine for ECS/EKS, eliminating the need to manage servers for containerized applications, ideal for microservices deployments."},
            {"name": "AWS Lambda", "justification": "Used for implementing smaller, event-driven microservices that can scale independently."},
            {"name": "Amazon DynamoDB", "justification": "Provides a scalable NoSQL database often used by microservices due to its performance and flexibility."},
            {"name": "Amazon API Gateway", "justification": "Acts as the entry point for microservices, providing API management and routing."},
            {"name": "AWS Database Migration Service (DMS)", "justification": "Migrates existing databases to support the new microservice architecture, potentially splitting monolithic databases."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A large manufacturing company wants to analyze equipment telemetry data in near real-time to predict failures. The data volume is high-velocity and continuous. What analytics pattern is best for processing and querying streaming data for operational insights?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests high-velocity, continuous telemetry data from manufacturing equipment."},
            {"name": "Amazon Kinesis Data Analytics", "justification": "Processes streaming data in near real-time using SQL or Apache Flink, enabling immediate anomaly detection and operational insights."},
            {"name": "Amazon DynamoDB", "justification": "Stores processed real-time data for low-latency queries and operational dashboards, providing high throughput and consistent performance."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Visualizes real-time insights for operational monitoring dashboards."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless batch processing system that can process large files (up to several GBs) asynchronously and then trigger downstream actions. Which serverless pattern is suitable for large file processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Serves as the durable and scalable storage for large files, acting as the trigger source for processing events."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers downstream Lambda functions or SQS queues when new large files are uploaded to S3."},
            {"name": "AWS Lambda", "justification": "Initiates and orchestrates the processing of large files. For very large files, Lambda can use services like SQS/Step Functions to break down work or trigger Fargate/Batch jobs."},
            {"name": "AWS Step Functions", "justification": "Orchestrates complex, multi-step workflows for processing large files, handling error management, retries, and sequential or parallel execution of tasks."},
            {"name": "AWS Batch (or AWS Fargate for containerized workloads)", "justification": "Processes the actual large files. Lambda can submit jobs to AWS Batch or Fargate for heavy lifting, as these services are better suited for long-running, resource-intensive tasks than direct Lambda execution limits."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a banking core system, all transactions must be processed with strict idempotency to prevent duplicate entries and ensure data integrity. Which architectural pattern guarantees exactly-once processing for financial transactions?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Acts as the initial entry point, capable of integrating with Lambda and supporting custom request validation."},
            {"name": "AWS Lambda", "justification": "Executes the transaction processing logic. It should be designed to be idempotent, meaning it produces the same result whether called once or multiple times."},
            {"name": "Amazon DynamoDB", "justification": "Used to store idempotency keys or transaction states. Before processing a transaction, Lambda checks if the idempotency key already exists in DynamoDB. If it does, the request is either ignored or the previous result is returned."},
            {"name": "Amazon SQS", "justification": "Can be used for asynchronous processing of transactions, with consumers designed for idempotency by checking message IDs or custom idempotency keys."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A new service will use generative AI to create personalized video advertisements. The video generation process is highly resource-intensive and can take a long time to complete. How can we manage these complex, long-running generative AI tasks efficiently and asynchronously?",
        "expected_answer_pattern": [
            {"name": "Amazon SQS", "justification": "Decouples the request submission from the video generation process, serving as a buffer for incoming personalization requests."},
            {"name": "AWS Step Functions", "justification": "Orchestrates the complex, long-running video generation workflow. It can manage multiple steps, including prompting the generative AI model, media processing, and final delivery, handling retries and state management efficiently."},
            {"name": "Amazon SageMaker", "justification": "Hosts the generative AI model for video content creation. Its asynchronous endpoints or batch transform jobs are suitable for long-running, resource-intensive tasks."},
            {"name": "AWS Elemental MediaConvert", "justification": "Processes and transcodes generated video content into various formats for different distribution channels."},
            {"name": "Amazon S3", "justification": "Stores input assets for video generation and the final generated video advertisements for delivery."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML model needs to be deployed in a highly scalable and cost-effective manner, capable of processing large volumes of data for inference periodically, without the need for real-time responsiveness. Which deployment pattern is suitable for batch inference?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Batch Transform", "justification": "A managed service for performing batch inference on large datasets. It automatically scales compute resources up and down, making it cost-effective for periodic, high-volume inference without real-time requirements."},
            {"name": "Amazon S3", "justification": "Serves as the input source for large datasets and the destination for the batch inference results, providing scalable and durable storage."},
            {"name": "AWS Step Functions", "justification": "Orchestrates the batch inference workflow, triggering SageMaker Batch Transform jobs based on schedules or data arrival, and handling subsequent actions."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise has many small applications that need to be migrated from on-premises to AWS. The goal is to move them quickly without extensive re-architecture. What migration pattern is suitable for a large number of simple applications?",
        "expected_answer_pattern": [
            {"name": "AWS Application Migration Service (AWS MGN)", "justification": "Automates the mass migration (re-hosting) of servers and applications to AWS, streamlining the process for a large number of simple applications by minimizing manual effort and downtime."},
            {"name": "Amazon EC2", "justification": "Provides the target compute environment for the migrated applications, allowing them to run 'as-is' in the cloud."},
            {"name": "Amazon EBS", "justification": "Provides durable block storage for the migrated EC2 instances."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A large-scale telemetry system needs to ingest billions of data points per day from IoT devices and perform real-time anomaly detection. The solution must handle high velocity and store data cost-effectively for long-term analysis. What analytics pattern is designed for massive IoT data ingestion and processing?",
        "expected_answer_pattern": [
            {"name": "AWS IoT Core", "justification": "Ingests billions of telemetry data points from IoT devices globally, managing device connections and authentication at scale."},
            {"name": "Amazon Kinesis Data Streams", "justification": "Acts as a highly scalable and durable buffer for ingesting high-velocity IoT data, providing real-time access for downstream processing."},
            {"name": "Amazon Kinesis Data Analytics", "justification": "Performs real-time processing and anomaly detection on the streaming IoT data using SQL or Apache Flink."},
            {"name": "Amazon S3", "justification": "Serves as the cost-effective data lake for long-term storage of raw and processed IoT telemetry data, supporting petabyte-scale retention."},
            {"name": "Amazon Athena", "justification": "Allows ad-hoc querying of the long-term data stored in S3 using standard SQL, without managing any servers."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless backend for a photo-sharing application that automatically resizes images upon upload. The image processing is asynchronous and should not block the user interface. Which serverless pattern supports asynchronous media processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores the original uploaded images and triggers an event notification upon new object creation."},
            {"name": "Amazon S3 Event Notifications", "justification": "Sends an event to AWS Lambda when a new image is uploaded, initiating the asynchronous processing."},
            {"name": "AWS Lambda", "justification": "Executes the image resizing logic. It's invoked asynchronously by S3 events, ensuring the user interface remains responsive. It also stores the resized images back into S3."},
            {"name": "Amazon DynamoDB", "justification": "Stores metadata about the images (original, resized versions, status), providing fast lookup for the application."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a financial analytics platform, we need to perform complex calculations on immutable ledger data to generate regulatory reports. The data needs to be verifiable and tamper-proof. Which specialized database pattern is suitable for such requirements?",
        "expected_answer_pattern": [
            {"name": "Amazon QLDB (Quantum Ledger Database)", "justification": "A fully managed ledger database that provides an immutable, cryptographically verifiable transaction log, ideal for financial ledger data where tamper-proofing and verifiability are paramount for regulatory reporting."},
            {"name": "AWS Lambda", "justification": "Triggers in response to changes in QLDB (via QLDB Streams) or on a schedule to perform complex calculations and generate reports."},
            {"name": "Amazon S3", "justification": "Stores the generated regulatory reports and can also serve as a data lake for large-scale archival and analysis of QLDB data."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "We are developing a code generation tool using a large language model. The generated code needs to adhere to specific style guides and best practices. How can we incorporate custom rules and constraints into the generative AI process to ensure quality output?",
        "expected_answer_pattern": [
            {"name": "Amazon Bedrock (or Amazon SageMaker for custom models)", "justification": "Provides the foundational large language model for code generation."},
            {"name": "AWS Lambda", "justification": "Serves as a post-processing layer. After initial code generation by the LLM, Lambda can invoke custom validation logic or integrate with tools to check adherence to style guides and best practices."},
            {"name": "Amazon CodeGuru (or custom linting/static analysis tools)", "justification": "Automatically reviews code for quality, identifies issues, and ensures adherence to coding standards, providing feedback for refinement."},
            {"name": "Amazon S3", "justification": "Stores generated code snippets and configuration for style guides or custom rules."},
            {"name": "AWS Step Functions", "justification": "Can orchestrate the multi-step process of code generation, validation, and refinement, allowing for conditional logic based on validation results."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML pipeline needs to monitor model performance in production, detect data drift, and alert when performance degrades. Which pattern supports continuous monitoring of ML models?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Model Monitor", "justification": "Automatically detects data drift, model quality degradation, and bias in deployed ML models, providing continuous monitoring in production."},
            {"name": "Amazon CloudWatch", "justification": "Collects metrics and logs from SageMaker Model Monitor, enabling creation of alarms and dashboards to alert when model performance degrades or drift is detected."},
            {"name": "Amazon S3", "justification": "Stores input and output data from model inference endpoints, which SageMaker Model Monitor uses for analysis."},
            {"name": "Amazon SNS", "justification": "Sends notifications (e.g., email, SMS) when CloudWatch alarms are triggered due to model degradation or drift."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to move its SAP ECC system to AWS. The goal is to perform a lift-and-shift migration of the SAP application and database. What migration pattern is specifically designed for SAP workloads?",
        "expected_answer_pattern": [
            {"name": "AWS Application Migration Service (AWS MGN)", "justification": "Used for re-hosting the SAP application servers to EC2 with minimal downtime and effort."},
            {"name": "AWS Database Migration Service (DMS)", "justification": "Migrates the SAP database (e.g., Oracle, SQL Server, SAP HANA) to a managed AWS database service like Amazon RDS or a self-managed database on EC2."},
            {"name": "Amazon EC2", "justification": "Hosts the SAP application servers and potentially the database server, configured to meet SAP's performance and certification requirements."},
            {"name": "Amazon EBS", "justification": "Provides high-performance block storage for SAP application and database servers on EC2."},
            {"name": "AWS Backint Agent (for SAP HANA)", "justification": "Integrates SAP HANA with Amazon S3 for backup and recovery, simplifying data management for SAP workloads."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A large-scale scientific research project needs to analyze petabytes of genomic data. The analysis involves complex computations that can be parallelized. What analytics pattern supports high-performance computing (HPC) for large datasets?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Provides scalable and cost-effective object storage for petabytes of raw genomic data, accessible by compute resources."},
            {"name": "AWS Batch", "justification": "Manages and scales compute resources for running parallelized, containerized batch jobs, ideal for complex genomic analysis workloads."},
            {"name": "AWS Step Functions", "justification": "Orchestrates complex multi-step analysis workflows, chaining together AWS Batch jobs and other services for genomic data processing."},
            {"name": "Amazon FSx for Lustre", "justification": "Provides a high-performance shared file system for workloads that require POSIX-compliant file access and high throughput, which can accelerate genomic computations."},
            {"name": "Amazon EC2 (especially Spot Instances)", "justification": "Provides the underlying compute capacity for AWS Batch, with Spot Instances offering significant cost savings for fault-tolerant HPC workloads."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to design a serverless application that can be deployed across multiple AWS regions and maintain data consistency globally, ensuring low-latency reads for users worldwide. Which serverless pattern supports global data consistency and low-latency access?",
        "expected_answer_pattern": [
            {"name": "Amazon DynamoDB Global Tables", "justification": "Provides multi-region, active-active replication for DynamoDB tables, ensuring low-latency reads and writes and automatic data consistency across regions for global applications."},
            {"name": "AWS Lambda", "justification": "Deploys application logic in multiple regions, accessing the nearest DynamoDB Global Table replica for low-latency interactions."},
            {"name": "Amazon Route 53", "justification": "Directs user traffic to the closest AWS region based on latency, ensuring users interact with the nearest application and database endpoint for optimal performance."},
            {"name": "Amazon API Gateway", "justification": "Deploys API endpoints in multiple regions, serving as the entry point for global users."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a blockchain-based financial application, we require a distributed ledger that provides immutability, transparency, and a verifiable transaction history. Which AWS service provides a managed blockchain solution?",
        "expected_answer_pattern": [
            {"name": "Amazon Managed Blockchain", "justification": "Provides a fully managed service for building and managing scalable blockchain networks using popular frameworks like Hyperledger Fabric or Ethereum, ensuring immutability, transparency, and verifiable transaction history for financial applications."},
            {"name": "Amazon QLDB (Quantum Ledger Database)", "justification": "While not a full blockchain, QLDB provides a cryptographically verifiable, immutable, and transparent transaction log, which can be used for financial ledger purposes where a centralized, trusted authority is acceptable."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to translate text between multiple languages. The translation process requires high accuracy and needs to be performed efficiently for diverse language pairs. What pattern facilitates multi-lingual text generation?",
        "expected_answer_pattern": [
            {"name": "Amazon Translate", "justification": "Provides high-quality neural machine translation for text between multiple languages, ensuring accuracy and efficiency for diverse language pairs."},
            {"name": "Amazon Bedrock (or Amazon SageMaker for custom models)", "justification": "Generates or processes text. Translate can be used as a pre-processing step for prompts or a post-processing step for generated text to handle multilingual requirements."},
            {"name": "AWS Lambda", "justification": "Orchestrates the translation workflow, invoking Amazon Translate as needed within the generative AI application flow."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to detect bias in training data and model predictions to ensure fairness and compliance. Which pattern supports bias detection and mitigation for machine learning?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Clarify", "justification": "Provides tools to detect potential bias in ML training data and models, and to help explain predictions, ensuring fairness and compliance."},
            {"name": "Amazon SageMaker Pipelines", "justification": "Integrates Clarify into the automated MLOps pipeline, allowing for continuous bias detection and mitigation throughout the model lifecycle."},
            {"name": "Amazon S3", "justification": "Stores training data and model artifacts, which Clarify uses for analysis."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An on-premises application has a custom-built reporting engine. We need to migrate this engine to AWS but also modernize it to use cloud-native analytics services. What migration strategy would you apply for this modernization?",
        "expected_answer_pattern": [
            {"name": "AWS Glue", "justification": "A serverless ETL service that can modernize the reporting engine's data processing capabilities, replacing custom scripts with managed services for data transformation and preparation."},
            {"name": "Amazon Redshift", "justification": "A fully managed, petabyte-scale cloud data warehouse that can replace the legacy reporting database, providing scalable and high-performance querying for reports."},
            {"name": "Amazon QuickSight", "justification": "A cloud-native business intelligence service that can visualize data from Redshift or other sources, replacing or extending the custom reporting frontend."},
            {"name": "AWS Database Migration Service (DMS)", "justification": "Migrates the underlying data from the on-premises database to Redshift or S3."},
            {"name": "AWS Step Functions (or AWS Lambda)", "justification": "Orchestrates the modernized reporting workflow, triggering Glue jobs and data loads."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global IoT fleet generates massive amounts of time-series data from sensors. We need to store this data efficiently and query it for trend analysis and anomaly detection. What specialized database pattern is suitable for time-series data?",
        "expected_answer_pattern": [
            {"name": "Amazon Timestream", "justification": "A fast, scalable, and serverless time-series database specifically optimized for ingesting, storing, and analyzing large volumes of time-series data from IoT devices, ideal for trend analysis and anomaly detection."},
            {"name": "AWS IoT Core", "justification": "Ingests data from the IoT fleet and routes it directly to Timestream."},
            {"name": "Amazon Kinesis Data Firehose", "justification": "Can be used as an intermediary to deliver streaming IoT data to Timestream, handling buffering and transformation."},
            {"name": "Amazon QuickSight", "justification": "Visualizes the time-series data stored in Timestream for trend analysis and dashboards."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless backend for a mobile gaming application that requires low-latency real-time updates for game state and player interactions. Which serverless pattern supports real-time multiplayer gaming?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway WebSocket APIs", "justification": "Provides a managed WebSocket endpoint for bi-directional, real-time communication between the mobile gaming clients and the backend."},
            {"name": "AWS Lambda", "justification": "Handles WebSocket messages, processing game state updates and player interactions with low latency."},
            {"name": "Amazon DynamoDB", "justification": "Stores mutable game state and player data, offering single-digit millisecond performance for high-throughput reads and writes required by real-time gaming."},
            {"name": "Amazon SQS (or Amazon EventBridge)", "justification": "Can be used for asynchronous processing of less critical game events or for fan-out messaging to multiple clients."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance reporting system in financial services, all data must be encrypted with FIPS 140-2 validated cryptographic modules. Which security pattern ensures FIPS compliance for encryption keys?",
        "expected_answer_pattern": [
            {"name": "AWS Key Management Service (KMS) with FIPS 140-2 validated Hardware Security Modules (HSMs)", "justification": "KMS uses FIPS 140-2 validated cryptographic modules to protect keys, ensuring compliance for encryption of sensitive data used in financial reporting."},
            {"name": "Amazon S3", "justification": "Stores data for compliance reporting, with server-side encryption enabled using KMS keys that are FIPS-compliant."},
            {"name": "Amazon RDS", "justification": "Encrypts data at rest using KMS keys for database-resident compliance data."},
            {"name": "AWS CloudTrail", "justification": "Logs all KMS API calls, providing an auditable trail of key usage for compliance reporting."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "We are building an application for personalized news summarization using a generative AI model. The solution needs to integrate with various news feeds and summarize articles in real-time. What pattern enables real-time news summarization?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests high-volume, real-time news feed data."},
            {"name": "AWS Lambda", "justification": "Processes news articles from Kinesis streams, extracts relevant text, and invokes the generative AI model for summarization."},
            {"name": "Amazon Bedrock (or Amazon SageMaker for custom models)", "justification": "Provides access to foundational models (LLMs) for real-time news summarization."},
            {"name": "Amazon DynamoDB", "justification": "Stores personalized summaries for quick retrieval by users, offering low-latency access at scale."},
            {"name": "Amazon S3", "justification": "Stores original news articles and larger summaries for archival or further analysis."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to support data ingestion from various sources, including databases, streaming data, and object storage. Which pattern ensures efficient and scalable data ingestion for ML?",
        "expected_answer_pattern": [
            {"name": "AWS Glue", "justification": "A serverless data integration service used for ETL, suitable for extracting and transforming data from various databases (RDS, S3) and preparing it for ML model training."},
            {"name": "Amazon Kinesis Data Firehose", "justification": "Ingests streaming data (e.g., from IoT devices, application logs) and reliably delivers it to S3, Redshift, or OpenSearch for ML processing."},
            {"name": "AWS DataSync", "justification": "Automates and accelerates the transfer of large datasets from on-premises storage to Amazon S3 for ML training and feature engineering."},
            {"name": "Amazon S3", "justification": "Serves as the central data lake for raw and processed ML datasets, supporting diverse data formats and petabyte-scale storage."},
            {"name": "Amazon RDS (or Amazon DynamoDB)", "justification": "Source databases from which data can be ingested for ML."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to consolidate multiple on-premises data sources into a central data lake on AWS. The data volumes are very large, and the migration needs to be highly automated and secure. What pattern is suitable for large-scale data ingestion during migration?",
        "expected_answer_pattern": [
            {"name": "AWS DataSync", "justification": "Automates and accelerates the transfer of large volumes of file and object data from on-premises to Amazon S3, ensuring high security and automation for mass data ingestion."},
            {"name": "AWS Snowball Edge (or Snowmobile for Exabyte-scale)", "justification": "A physical appliance for moving extremely large datasets (petabytes to exabytes) to AWS when network bandwidth is limited, suitable for initial large-volume data ingestion."},
            {"name": "AWS Database Migration Service (DMS)", "justification": "Migrates databases from on-premises to AWS, supporting large-scale data ingestion for database content into the data lake (e.g., S3)."},
            {"name": "Amazon S3", "justification": "The highly scalable and durable destination for the central data lake, storing ingested data."},
            {"name": "AWS PrivateLink (with VPC Endpoints)", "justification": "Ensures secure, private connectivity for data transfer services to S3 within the VPC, enhancing security for data ingestion."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A large-scale content platform needs to analyze user engagement with articles and videos across various devices. The data needs to be aggregated and normalized for comprehensive analytics. What analytics pattern supports cross-device user engagement analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests high-volume, real-time user engagement data from various devices."},
            {"name": "AWS Lambda", "justification": "Processes Kinesis stream records, performing real-time aggregation and normalization of engagement data."},
            {"name": "Amazon Redshift", "justification": "A cloud data warehouse optimized for complex analytical queries over large datasets, used for storing aggregated and normalized user engagement data for comprehensive analysis."},
            {"name": "Amazon S3", "justification": "Serves as the data lake for raw and processed engagement data, providing scalable and cost-effective storage."},
            {"name": "AWS Glue", "justification": "Used for batch ETL jobs to further transform and normalize data in S3 for Redshift, or for building a unified schema across devices."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless backend that exposes a public API but uses private, internal services for business logic. How would you design a serverless architecture to bridge public API access with private backend processing?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Exposes the public REST API endpoints to external clients."},
            {"name": "AWS Lambda", "justification": "Serves as the compute layer triggered by API Gateway. These functions operate within a VPC to access private internal services."},
            {"name": "Amazon VPC (Virtual Private Cloud)", "justification": "Hosts the internal private services and the Lambda functions, providing network isolation and security."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your private internal services. This bridges the public API with the private backend."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For an investment portfolio management system, real-time market data needs to be securely ingested and processed with very low latency to inform trading decisions. Which architectural pattern ensures secure, high-throughput market data ingestion?",
        "expected_answer_pattern": [
            {"name": "Amazon Kinesis Data Streams", "justification": "Ingests high-volume, real-time market data with high throughput and low latency, acting as a buffer for continuous data streams."},
            {"name": "AWS Lambda", "justification": "Processes Kinesis stream records in near real-time, performing transformations or aggregations before storing data for trading decisions. Lambda can operate within a VPC."},
            {"name": "Amazon VPC", "justification": "Provides network isolation for Lambda functions and other processing resources, ensuring secure ingestion and processing of sensitive market data."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts data in transit within Kinesis streams and at rest in downstream storage, ensuring data security."},
            {"name": "Amazon DynamoDB", "justification": "Stores processed market data for low-latency queries by the trading system, offering high throughput and consistent performance."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to analyze legal documents and extract specific entities (e.g., contract dates, parties). The solution must handle large document volumes and ensure accuracy. What pattern supports structured information extraction from unstructured text?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores large volumes of legal documents for processing."},
            {"name": "Amazon Textract", "justification": "Extracts text and data from documents (including forms and tables), converting unstructured legal documents into structured, machine-readable data."},
            {"name": "Amazon Comprehend", "justification": "Performs natural language processing (NLP) to identify entities, key phrases, and relationships within the extracted text, ensuring accuracy for specific entity extraction."},
            {"name": "Amazon Bedrock (or Amazon SageMaker for custom models)", "justification": "A foundational model that can be used for advanced entity resolution, normalization, or summarization after initial extraction by Textract/Comprehend."},
            {"name": "AWS Lambda", "justification": "Orchestrates the document processing workflow, triggering Textract, then Comprehend, and then potentially Bedrock for further refinement."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to be deployed and managed in a consistent and repeatable manner across different environments (development, staging, production). Which pattern supports automated ML model deployment automation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker Model Registry", "justification": "Centralizes model versions, metadata, and approval statuses, ensuring consistent model management across environments."},
            {"name": "AWS CodePipeline", "justification": "Orchestrates the automated CI/CD pipeline for ML models, integrating various stages from model training to deployment."},
            {"name": "AWS CodeBuild", "justification": "Builds and tests model deployment packages."},
            {"name": "AWS CodeDeploy", "justification": "Automates the deployment of ML models to SageMaker endpoints or other inference targets, ensuring consistent and repeatable deployments across environments."},
            {"name": "AWS Step Functions", "justification": "Can be used to orchestrate complex deployment strategies, like canary deployments or blue/green, triggered by CodePipeline."},
            {"name": "Amazon SageMaker Endpoints", "justification": "Serves as the target for automated model deployments, managing the inference serving infrastructure."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise has critical applications that require near-zero downtime during migration to AWS. What migration strategy and associated pattern would you apply to achieve minimal disruption?",
        "expected_answer_pattern": [
            {"name": "AWS Application Migration Service (AWS MGN)", "justification": "Automates the replication of source servers to AWS with continuous data synchronization, allowing for rapid cutover with near-zero downtime."},
            {"name": "AWS Database Migration Service (DMS)", "justification": "Enables continuous, low-latency replication of databases from on-premises to AWS, supporting database migration with minimal downtime."},
            {"name": "Amazon Route 53", "justification": "Manages DNS routing to gradually shift traffic from on-premises applications to AWS endpoints, allowing for controlled cutovers and minimizing disruption."},
            {"name": "Amazon EC2", "justification": "Provides the target compute environment for replicated servers, allowing them to be brought up quickly with the latest data."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A financial institution needs to analyze historical trading data to identify market trends and perform backtesting of trading strategies. The dataset is massive and requires complex SQL queries. What analytics pattern supports historical trading data analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Provides highly scalable, durable, and cost-effective storage for massive historical trading datasets."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that discovers, transforms, and loads historical trading data from S3 into Amazon Redshift, preparing it for complex analysis."},
            {"name": "Amazon Redshift", "justification": "A fully managed, petabyte-scale cloud data warehouse optimized for complex SQL queries over large datasets, providing high performance for historical trading data analysis and backtesting."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows ad-hoc SQL queries directly on data in S3 for flexible exploration without loading it into Redshift."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless API that handles file uploads and storage, ensuring that the uploaded files are scanned for malware before being stored. Which serverless pattern supports secure file uploads with content scanning?",
        "expected_answer_pattern": [
            {"name": "Amazon API Gateway", "justification": "Provides a secure endpoint for users to initiate file uploads (e.g., by requesting a pre-signed S3 URL)."},
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Generated by a Lambda function (via API Gateway), allowing users to securely upload files directly to S3 without exposing credentials."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers an event to AWS Lambda when a new file is uploaded to a 'quarantine' S3 bucket."},
            {"name": "AWS Lambda", "justification": "Invoked by S3 event notifications, this function triggers the malware scan and moves the file to a 'clean' or 'quarantined' bucket based on the scan result."},
            {"name": "Amazon GuardDuty (or third-party anti-malware service integrated with Lambda)", "justification": "Detects threats and suspicious activity for S3 buckets, or a Lambda-triggered anti-malware solution scans the uploaded file content."},
            {"name": "Amazon SQS", "justification": "Can be used as a buffer between the S3 event and the Lambda function for greater resilience and retry capability."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to S3, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a regulatory reporting system in financial services, all data must be audited for compliance, and changes to data schemas must be managed version-controlled. Which data governance pattern ensures auditable data changes and schema versioning?",
        "expected_answer_pattern": [
            {"name": "Amazon QLDB (Quantum Ledger Database)", "justification": "Provides an immutable, cryptographically verifiable transaction log for all data changes, creating an auditable history for compliance."},
            {"name": "AWS Lake Formation", "justification": "Simplifies setting up secure data lakes and centralizes the management of data access, fine-grained permissions, and schema management for compliance reporting."},
            {"name": "AWS Glue Data Catalog", "justification": "Acts as a centralized metadata repository for all data in the data lake, including schema definitions. It supports schema versioning and lineage, crucial for managing schema changes in a regulated environment."},
            {"name": "AWS CloudTrail", "justification": "Logs all API calls made to Glue Data Catalog and Lake Formation, providing an audit trail of schema changes and access patterns."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    },
    {
        "question": "We need to build a serverless web application that supports user uploads of large files and uses an internal image processing service. The solution must be highly available and scalable. Which serverless pattern ensures efficient large file upload and processing?",
        "expected_answer_pattern": [
            {"name": "Amazon S3 Pre-signed URLs", "justification": "Allows the web application to provide users with temporary, secure access to upload large files directly to S3 without exposing AWS credentials, ensuring scalability for uploads."},
            {"name": "Amazon S3 Event Notifications", "justification": "Triggers a serverless function (Lambda) when a new large file is uploaded to a 'quarantine' S3 bucket, initiating the processing workflow."},
            {"name": "AWS Lambda", "justification": "Orchestrates the image processing. For very large files, Lambda might trigger a larger compute service like AWS Batch or AWS Fargate within a VPC."},
            {"name": "Amazon VPC Link", "justification": "Enables API Gateway (if used as frontend) to securely connect to private Network Load Balancers or Application Load Balancers within your VPC, which front your internal image processing service."},
            {"name": "AWS Fargate (or AWS Batch)", "justification": "Provides serverless compute for the resource-intensive internal image processing service, scaling automatically without server management, and operating within a private VPC."}
        ],
        "relevant_lens": "Serverless Application"
    },
    {
        "question": "For a compliance system in financial services, all audit records must be signed and timestamped to prove their integrity and non-tampering. Which security pattern ensures cryptographic integrity of audit logs?",
        "expected_answer_pattern": [
            {"name": "AWS CloudTrail", "justification": "Records API activity across your AWS account. CloudTrail logs can be delivered to S3 and are integrity-validated using SHA-256 hashing and SHA-256 with RSA for digital signing, proving non-tampering."},
            {"name": "Amazon S3", "justification": "Stores CloudTrail logs with versioning and S3 Object Lock (WORM) enabled, ensuring immutability of audit records."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Encrypts CloudTrail logs at rest in S3, adding a layer of security."},
            {"name": "Amazon GuardDuty", "justification": "Continuously monitors for malicious activity and unauthorized behavior, including potential tampering with audit logs."}
        ],
        "relevant_lens": "Financial Services Industry"
    },
    {
        "question": "A generative AI application needs to generate realistic human faces or avatars for a virtual reality experience. The process is computationally intensive and requires specialized rendering capabilities. What pattern supports realistic human avatar generation?",
        "expected_answer_pattern": [
            {"name": "Amazon SageMaker", "justification": "Hosts and manages the deployment of highly complex generative AI models (e.g., GANs, diffusion models) for face/avatar generation, leveraging specialized GPU instances."},
            {"name": "Amazon EC2 with specialized instances (e.g., P-instances with GPUs)", "justification": "Provides the underlying compute power for training and potentially large-scale batch inference of realistic avatar generation, offering the necessary GPU resources. These can be managed by AWS Batch."},
            {"name": "Amazon S3", "justification": "Stores input data (e.g., reference images, parameters) and the generated high-resolution 3D models or image sequences for avatars."},
            {"name": "AWS Batch", "justification": "Orchestrates and scales the computationally intensive batch rendering or generation tasks, automatically provisioning and managing EC2 instances with GPUs."},
            {"name": "AWS Step Functions", "justification": "Manages the multi-step workflow for complex avatar generation, including triggering SageMaker or Batch jobs, and handling state."}
        ],
        "relevant_lens": "Generative AI"
    },
    {
        "question": "Our ML models need to operate on encrypted data without decrypting it, to protect sensitive information during inference. Which pattern supports homomorphic encryption or similar privacy-preserving ML techniques?",
        "expected_answer_pattern": [
            {"name": "AWS Nitro Enclaves", "justification": "Provides an isolated compute environment within an EC2 instance to process sensitive data, protecting it even from privileged users on the host instance, enabling privacy-preserving computation for ML inference."},
            {"name": "AWS Key Management Service (KMS)", "justification": "Manages the encryption keys used for data at rest and in transit, and potentially for homomorphic encryption schemes."},
            {"name": "Amazon SageMaker", "justification": "Deploys ML models within an EC2 instance that can be configured with Nitro Enclaves for confidential inference."},
            {"name": "Amazon S3", "justification": "Stores encrypted input data and encrypted inference results."}
        ],
        "relevant_lens": "Machine Learning"
    },
    {
        "question": "An enterprise wants to migrate a large, petabyte-scale data archive from on-premises tape libraries to AWS. The goal is cost-effective long-term storage with occasional retrieval. What migration pattern addresses large-scale archive migration?",
        "expected_answer_pattern": [
            {"name": "AWS Snowball Edge (Storage Optimized)", "justification": "A physical device used to securely transfer petabytes of data from on-premises tape libraries to AWS, overcoming network limitations and ensuring secure, offline data transfer."},
            {"name": "Amazon S3 Glacier Deep Archive", "justification": "Provides the lowest-cost cloud storage class for long-term archiving of data that is rarely accessed, ideal for replacing tape libraries."},
            {"name": "AWS DataSync", "justification": "Can be used for online migration of file data from on-premises to S3 or Glacier, if network bandwidth allows, automating and accelerating transfers."}
        ],
        "relevant_lens": "Migration"
    },
    {
        "question": "A global media company needs to perform interactive analytics on vast amounts of video streaming logs to understand viewer engagement. The data is semi-structured and needs flexible querying. What analytics pattern supports interactive log analysis?",
        "expected_answer_pattern": [
            {"name": "Amazon S3", "justification": "Stores vast amounts of semi-structured video streaming logs cost-effectively as a data lake."},
            {"name": "AWS Glue", "justification": "A serverless ETL service that can be used to crawl the semi-structured logs in S3, infer schemas, and create a searchable data catalog."},
            {"name": "Amazon Athena", "justification": "A serverless interactive query service that allows flexible SQL queries directly on the semi-structured video streaming logs stored in S3, leveraging the Glue Data Catalog, without managing any servers."},
            {"name": "Amazon QuickSight (or Amazon Managed Grafana/OpenSearch Dashboards)", "justification": "Provides interactive dashboards and visualization capabilities for analyzing the queried data from Athena or OpenSearch Service."}
        ],
        "relevant_lens": "Analytics"
    }
]